{
  
    
        "post0": {
            "title": "When will the United Kingdom be Fully Vaccinated Against Covid-19?",
            "content": "The Covid-19 pandemic has caused catastrophic losses and changes to life worldwide since the first reported cases in December 2019. A year later, in December 2020, Covid-19 vaccines began to be approved for general public use in countries worldwide, after an unprecedented speed and scale of development and clinical trials (for a process that typically take closer to a decade to complete). Vaccines have therefore emerged as the shining light at the end of the tunnel that we all hope will lead to the end of the days of lockdown measures and hospitals filled to capacity. . So the question on the tip of our tongues in 2021 has been &quot;when will I, my family, my country and the world be vaccinated against Covid-19?&quot; In this data story we&#39;ll be focusing on the country-level question, with the objective of estimating when the adult population in the United Kingdom (UK) will be fully vaccinated against Covid-19. We&#39;ll collect data on the progress of the UK&#39;s vaccination programme so far, and then explore several approaches for forecasting how this might continue in the future. . A brief introduction to vaccinations in the UK: The Pfizer/BioNTech vaccine, which uses new mRNA vaccine technology, was given emergency approval on 2nd December 2020, with the first dose being administered to 91-year-old Margaret Keenan on 8th December. Since that date vaccines have continued to be rolled out in an increasing number of locations nationwide, including GP surgeries, hospitals, pharmacies and dedicated vaccination centres. The Oxford/AstraZeneca vaccine, based on more traditional vaccine technology, began to be administered around a month later on 4th January, and the Moderna mRNA vaccine more recently on 7th April 2021. All three vaccines require two doses, typically spaced between one and three months apart. Priority for vaccination is given to those at highest risk from Covid-19, namely healthcare workers, the elderly, and clinically vulnerable people. We expand on the details of the vaccination programme throughout this data story. . Note that the story is written based on the status of the vaccination programme on 4th May 2021. . Setup . For this analysis we&#39;ll be using the requests and json libraries for interacting with data from the internet, numpy, pandas and datetime for manipulating and analysing data and dates, and matplotlib and seaborn for visualising data. In the cell below we also configure some of the defaults for the style of matplotlib figures, in particular increasing the font size. . import requests import json # Analyse data and dates import numpy as np import pandas as pd from datetime import datetime, timedelta # Visualisations import matplotlib.pyplot as plt import matplotlib import seaborn as sns plt.style.use(&quot;seaborn-notebook&quot;) fontsize = 14 plt.rc(&quot;font&quot;, size=fontsize) plt.rc(&quot;axes&quot;, titlesize=fontsize) plt.rc(&quot;axes&quot;, labelsize=fontsize) plt.rc(&quot;xtick&quot;, labelsize=fontsize) plt.rc(&quot;ytick&quot;, labelsize=fontsize) plt.rc(&quot;legend&quot;, fontsize=fontsize) width = 12 height = 5 figsize = (width, height) %matplotlib inline . Getting the Data . The government has developed a Coronavirus dashboard which is updated daily with key statistics including the number of Covid-19 tests, cases, hospitalisations, deaths and vaccinations. You can view the dashboard here: https://coronavirus.data.gov.uk/ . All of the source data can be downloaded, for example as a spreadsheet, by clicking the &quot;download&quot; button under each table or graph. Even better for our purposes is that the dashboard has an &quot;Application Programming Interface&quot; (API), which we can use to extract precisely the data we&#39;re interested in programmatically for this notebook! . The documentation for the API is found under the &quot;Developer&#39;s Guide&quot; section of the dashboard, which describes the various parameters that are available and how they should be configured. In particular, there are Python examples which show us how to structure a data query. There are three main steps, as follows: . Step 1: Defining the geographical region we are interested in. Some of the data is available regionally as well as nationally. The number of positive cases, for example, is available both for the UK as a whole but also down to &quot;local authority&quot; level (local council areas, of which there are over 300 in England alone). It&#39;s possible to get vaccine data for each of the four nations in the UK separately, but for this story we&#39;ll only consider the UK as a whole. . In the API, to define that we wish to retrieve summary data for the whole UK we must set the areaType parameter to be the value &quot;overview&quot;, as follows: . filters = &quot;areaType=overview&quot; . Step 2: Defining which data we wish to retrieve. To explore the status of the vaccination programme we would like to know the number of new first and second vaccine doses administered on each day, and the overall (cumulative) total of first and second doses administered up to and including each day. . In the API these fields are given long names like newPeopleVaccinatedFirstDoseByPublishDate. Finding the correct names for the data you&#39;re interested can be a bit tricky. Most are listed in the documentation but not all - the vaccination data has been added fairly recently and isn&#39;t documented, for example. Usually the field names for the API match the column names in files downloaded from the website, so you can do some super sleuthing and find them that way if you&#39;d like to try an analysis with different data! . As the names are long, the API helpfully let&#39;s us rename them to something more convenient using the dictionary format seen below. We&#39;ll call the cumulative totals cumFirst and cumSecond, and the new doses each day newFirst and newSecond. Finally, to pass this dictionary to the API it must be converted into a string without any spaces, which we achieve using the json.dumps function. . structure = { &quot;date&quot;: &quot;date&quot;, &quot;newFirst&quot;: &quot;newPeopleVaccinatedFirstDoseByPublishDate&quot;, &quot;cumFirst&quot;: &quot;cumPeopleVaccinatedFirstDoseByPublishDate&quot;, &quot;newSecond&quot;: &quot;newPeopleVaccinatedSecondDoseByPublishDate&quot;, &quot;cumSecond&quot;: &quot;cumPeopleVaccinatedSecondDoseByPublishDate&quot;, } # convert the dictionary into a string without spaces (using the separators argument) structure = json.dumps(structure, separators=(&quot;,&quot;, &quot;:&quot;)) . Step 3: Submitting the API query. We can now package our parameters up into the structure required by the API, and send our query using the requests.get function. If the query is successful we should see a status code of 200 (as per the convention for request status codes). If not it should also come back with a useful error message to diagnose the problem with our query. . ENDPOINT = &quot;https://api.coronavirus.data.gov.uk/v1/data&quot; api_params = {&quot;filters&quot;: filters, &quot;structure&quot;: structure} response = requests.get(ENDPOINT, params=api_params, timeout=10) if response.status_code != 200: request_failed = True print(f&quot;Request failed: { response.text }&quot;) else: request_failed = False . Our query was successful, but where is the data? We can convert the data the request contains into a Python function using the .json() method on the response, and then look at what fields are contained in the data (the keys of the dictionary): . if request_failed: # load previously saved data if API request failed with open(&quot;20210517_data.json&quot;) as f: j = json.load(f) else: j = response.json() print(j.keys()) . dict_keys([&#39;length&#39;, &#39;maxPageLimit&#39;, &#39;totalRecords&#39;, &#39;data&#39;, &#39;requestPayload&#39;, &#39;pagination&#39;]) . This doesn&#39;t look like the vaccination data we&#39;re expecting yet. The response also includes metadata about our query, in particular whether we queried too much data to return in one go. If the &quot;length&quot; of our query was larger than the &quot;maxPageLimit&quot; we&#39;d have to split our query into multiple queries. . print(j[&quot;length&quot;], j[&quot;maxPageLimit&quot;]) . 127 2500 . In this case we are a long way under the limit so we should have all the data. To find it we can have a look at one of the elements in the &quot;data&quot; list: . print(j[&quot;data&quot;][0]) . {&#39;date&#39;: &#39;2021-05-16&#39;, &#39;newFirst&#39;: 131318, &#39;cumFirst&#39;: 36704672, &#39;newSecond&#39;: 183745, &#39;cumSecond&#39;: 20287403} . Each element of the &quot;data&quot; list contains the vaccination data for one day. To make it easier to analyse the data we can convert it to a pandas data frame, using the date as the unique index for the rows (as we have one row for each day). We also take care to properly convert the date string representations into actual Python datetimes, so we can benefit from pandas&#39; features for processing time series data. We also convert the numbers into units of 1 million to make them easier to read in figures and tables later. . df = pd.DataFrame(j[&quot;data&quot;]) # use the &quot;date&quot; column to index our data df.set_index(&quot;date&quot;, inplace=True) # convert the date text strings into Python datetimes df.index = pd.to_datetime(df.index) # sort the data from oldest to newest df.sort_index(inplace=True) # convert all totals to millions df = df / 1e6 df.tail() . newFirst cumFirst newSecond cumSecond . date . 2021-05-12 0.184210 | 35.906671 | 0.452437 | 18.890969 | . 2021-05-13 0.209284 | 36.115955 | 0.428041 | 19.319010 | . 2021-05-14 0.220068 | 36.336023 | 0.393402 | 19.712412 | . 2021-05-15 0.237331 | 36.573354 | 0.391246 | 20.103658 | . 2021-05-16 0.131318 | 36.704672 | 0.183745 | 20.287403 | . Later on, it will also be helpful to have data on the total number of doses given, i.e. the number of first doses plus the number of second doses. Pandas let&#39;s us quickly create new columns for these values, as follows: . df[&quot;newTot&quot;] = df[&quot;newFirst&quot;] + df[&quot;newSecond&quot;] df[&quot;cumTot&quot;] = df[&quot;cumFirst&quot;] + df[&quot;cumSecond&quot;] . Our query above gets all the data up to the current date. To ensure re-running the notebook reproduces the same results from the time of writing only data up to 4th May should be included, which is done using the run_as_date variable below. If you&#39;d like to see the latest results instead you can change the value to today&#39;s date. . # (run_as_date = datetime(2021, 5, 4)), or use today&#39;s date to update # the results with the latest available data (run_as_date = datetime.now()). run_as_date = datetime(2021, 5, 4) # Latest data at time of publishing (vs. initial time of writing as above) # Used to compare forecasts with actual data at the end of the story publish_date = datetime(2021, 5, 16) publish_date_data = df.loc[publish_date] # filter the data to only include dates up to the run_as_date df = df[df.index &lt; run_as_date] . Vaccines So Far . Now we have the data we need to start forecasting the future of the vaccination programme, but before we jump in it&#39;s always a good idea to explore the historical data first. Let&#39;s have a look at the first row of our data (the earliest date we have data for): . df.iloc[0] . newFirst NaN cumFirst 2.286572 newSecond NaN cumSecond 0.391399 newTot NaN cumTot 2.677971 Name: 2021-01-10 00:00:00, dtype: float64 . The last row of our data (the last date we have data for): . df.iloc[-1] . newFirst 0.072600 cumFirst 34.667904 newSecond 0.127499 cumSecond 15.630007 newTot 0.200099 cumTot 50.297911 Name: 2021-05-03 00:00:00, dtype: float64 . And finally the mean (average) number of doses administered per day: . df[&quot;newFirst&quot;].mean(), df[&quot;newSecond&quot;].mean() . (0.2865604601769912, 0.13485493805309734) . Although the first Covid-19 vaccine in the UK (outside of clinical trials) was administered on 8th December 2020, the daily data that we have starts on 10th January 2021, by which time 2.29 million first doses and 0.39 million second doses had been given. Since that date a mean (average) of 0.29 million (290,000) first doses and 0.13 million (130,000) second doses have been administered per day. As of 3rd May 2021, a total of 34.7 million people have been vaccinated with a first dose and 15.6 million with a second dose, which we can see by looking at the last row of the data. . To make sense of these numbers we are missing one more piece of data - how many people are there to vaccinate? We can find this in the government&#39;s vaccine delivery plan. The plan specifies that the UK adult population (children are not currently being vaccinated) is approximately 53 million people, as well giving the sub-totals for the nine priority groups who will receive vaccines first. Government targets typically focus on three goals - vaccinating the 15 million people in priority groups 1-4 (the over 70s, care home residents, and health &amp; social care workers), the 32 million people in the groups 1-9 (additionally including over 50s and the clinically vulnerable), and the whole adult population. . Comparing to the values above, we can see that the number of people vaccinated so far exceeds the population of groups 1-9 for first doses, and of groups 1-4 for second doses. However, this doesn&#39;t mean everyone in those groups has been vaccinated. Although take up rates of the vaccine in the UK are high, there is regional variation in the speed of the rollout and take up rates are lower in certain sub-groups, such as for ethnic minorities, poorer areas, and care home staff. The population in each priority group is also estimated as the groups can be overlapping. For example, a clinically vulnerable, 50 year-old social care worker meets the criteria for priority groups 2, 6 and 9. Further statistics on take up rates can be found in NHS England data. . Bearing all the above in mind, the priority group population totals are still helpful to give a rough estimate of progress through the vaccination programme so let&#39;s save them for future use: . priority_totals = {&quot;Groups 1-4&quot;: 15, &quot;Groups 1-9&quot;: 32, &quot;All Adults&quot;: 53} . It&#39;s helpful to visualise the data, so before continuing we will do a bit of work here to set up plotting functions we can re-use throughout our analysis. First let&#39;s define colours and labels to use for the data columns in all the figures we create. These are in the col_format dictionary below, which all our plotting functions can access. . Then we define a function plot_column, which uses the pandas plot function to plot a column from the dataset in our chosen style. It also includes options to display a weekly rolling average of the data, rather than the original raw data, calculated using the statement df[column].rolling(window=7).mean() below (where window=7 means we calculate the average across 7 days). Finally, it will be helpful to distinguish between actual historical vaccine data, and estimates from the forecasts we create. If the forecast_date argument is defined the data after that date will be plotted with a dashed line instead of a solid line. The data after that date is selected using the statement data[data.index &gt;= forecast_date]. . col_format = { &quot;cumFirst&quot;: {&quot;label&quot;: &quot;1st&quot;, &quot;color&quot;: &quot;orange&quot;}, &quot;cumSecond&quot;: {&quot;label&quot;: &quot;2nd&quot;, &quot;color&quot;: &quot;deepskyblue&quot;}, &quot;cumTot&quot;: {&quot;label&quot;: &quot;Total&quot;, &quot;color&quot;: &quot;k&quot;}, &quot;newFirst&quot;: {&quot;label&quot;: &quot;1st&quot;, &quot;color&quot;: &quot;orange&quot;}, &quot;newSecond&quot;: {&quot;label&quot;: &quot;2nd&quot;, &quot;color&quot;: &quot;deepskyblue&quot;}, &quot;newTot&quot;: {&quot;label&quot;: &quot;Total&quot;, &quot;color&quot;: &quot;k&quot;}, } def plot_column( df, column, ax, forecast_date=None, rolling=False, forecast_label=&quot;Forecast&quot;, **kwargs ): &quot;&quot;&quot; Plot a column in a data frame, optionally calculating its rolling weekly average and distinguishing between actual and forecasted data. df - vaccination data frame column - column of df to plot ax - matplotlib axis to use for the plot forecast_date - plot data from this date with a dashed line rolling - plot rolling weekly average instead of raw data forecast_label - label given to forecast data in plot legend **kwargs - additional arguments passed to pandas plotting function &quot;&quot;&quot; color = col_format[column][&quot;color&quot;] label = col_format[column][&quot;label&quot;] if rolling: data = df[column].rolling(window=7).mean() label = label + &quot; (7d avg)&quot; else: data = df[column] if forecast_date is None: data.plot(color=color, label=label, ax=ax, **kwargs) else: # plot actual data with solid line data[data.index &lt;= forecast_date].plot( color=color, label=label, ax=ax, linestyle=&quot;-&quot;, linewidth=3, **kwargs ) # plot forecast data with dashed line data[data.index &gt;= forecast_date].plot( color=color, label=forecast_label + &quot; &quot; + label, ax=ax, linestyle=&quot;--&quot;, **kwargs ) . As well as showing the number of doses, we&#39;d like to show when the number of people vaccinated (with 1st or 2nd doses) exceeds the total of each priority sub-group. For each group, we can find the date this happened using the statement df[df[column] &gt;= (pop - tol)].index[0], where pop is the population of the sub-group, column is either cumFirst or cumSecond (depending on whether we want to calculate it for 1st or 2nd doses), and tol is a small value used to account for any rounding errors in the forecasts we do later. . The annotate_group_completions function below can then be used to add horizontal and vertical lines and text labels to a plot for those dates. As discussed earlier, we must remember that it&#39;s not true that everyone in the priority group will have been vaccinated by the dates we calculate here, rather they are general indicators of overall progress. . def annotate_group_completions(df, column, ax, text_offset=1.02, tol=1e-7): &quot;&quot;&quot; Add text labels and lines indicating the dates the number of people vaccinated exceded the population of priority sub-groups. df - vaccination data frame column - column to add labels for, either cumFirst or cumSecond ax - matplotlib axix to add labels to text_offset - space between data point and text label tol - define group to be vaccinated if total vaccines is within tol of its population &quot;&quot;&quot; label = col_format[column][&quot;label&quot;] max_col = df[column].max() for name, pop in priority_totals.items(): if max_col &gt;= (pop - tol): # vaccines completed for this group complete_date = df[df[column] &gt;= (pop - tol)].index[0] ax.hlines(pop, 0, complete_date, color=&quot;k&quot;, linewidth=0.5) ax.vlines(complete_date, 0, pop, color=&quot;k&quot;, linewidth=0.5) ax.text( complete_date, pop * text_offset, f&quot;{name} n{label} Doses n{complete_date.date()}&quot;, ha=&quot;center&quot;, size=12, fontweight=&quot;bold&quot;, ) # extend x-axis if text label is near the end of the axis if df.index.max() - complete_date &lt; timedelta(days=7): ax.set_xlim(df.index.min(), complete_date + timedelta(days=7)) . Now we are almost there! The plot_cumulative_doses function below uses the two previous functions to construct a figure for the cumulative total of first and second doses: . def plot_cumulative_doses( df, forecast_date=None, figsize=figsize, title=None, forecast_label=&quot;Forecast&quot; ): &quot;&quot;&quot; Plot cumulative first and second doses, and the dates when the vaccination of prioriy groups completed. Optionally distinguish actual data and forecasted data. Data after forecast_date will be displayed with a dashed line. &quot;&quot;&quot; fig, ax = plt.subplots(1, 1, figsize=figsize) for col in [&quot;cumFirst&quot;, &quot;cumSecond&quot;]: plot_column( df, col, ax, forecast_date=forecast_date, forecast_label=forecast_label, ) annotate_group_completions(df, col, ax) # set axis limits, titles and legend ymin = 0 ymax = df[&quot;cumFirst&quot;].max() * 1.15 ax.spines[&#39;top&#39;].set_visible(False) ax.set_ylim(ymin, ymax) ax.set_ylabel(&quot;Number of Doses [millions]&quot;) ax.set_xlabel(&quot;&quot;) ax.legend(loc=&quot;upper left&quot;) if title is not None: ax.set_title(title, fontsize=18) # add a secondary y-axis showing the % of the population vaccinated right_ax = ax.twinx() right_ax.spines[&#39;top&#39;].set_visible(False) uk_pop = priority_totals[&quot;All Adults&quot;] y_perc_min = 100 * ymin / uk_pop y_perc_max = 100 * ymax / uk_pop right_ax.set_ylim(y_perc_min, y_perc_max) right_ax.set_yticks(range(int(y_perc_min), min(int(y_perc_max), 110), 10)) right_ax.set_ylabel(&quot;% Adults Vaccinated&quot;) return ax . The following figure shows the history of the total number of people vaccinated with first and second doses, and the dates when priority groups were vaccinated: . plot_cumulative_doses(df, title=f&quot;UK Vaccinations up to {df.index.date.max()}&quot;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;UK Vaccinations up to 2021-05-03&#39;}, ylabel=&#39;Number of Doses [millions]&#39;&gt; . As stated in Boris Johnson&#39;s address to the nation on 4th January 2021, the first target the government set was to offer the first dose of a vaccine to everyone in the top four priority groups by mid-February. As we can see above this target was met on 13th February. The next target set by the government was then to offer a first dose to the first nine priority groups (the over 50s) with a first dose by the end of April. This was somewhat pessimistic given the vaccination rates at the time, and was ultimately completed a couple of weeks earlier on 9th April. Overall, the start of the UK vaccination programme has been very successful and amongst the fastest in the world. . It&#39;s also interesting to look at the daily number of new doses each day, and this is particularly relevant for forecasting future doses. As there&#39;s quite a large variation in the day to day totals (more on this later) it&#39;s helpful to look at weekly rolling averages as well. We&#39;ll also display the combined total of 1st and 2nd doses administered on each day (newTot), as this gives the best representation of the overall vaccine supply and capacity to administer vaccines. . We can create a similar function to do this: . def plot_daily_doses( df, forecast_date=None, show_daily=True, show_rolling=True, figsize=figsize, title=None, ): &quot;&quot;&quot; Plot daily first doses, second doses, the sum of 1st and 2nd doses, and their weekly rolling averages. &quot;&quot;&quot; # figure properties fig, ax = plt.subplots(1, 1, figsize=figsize) ax.spines[&quot;right&quot;].set_visible(False) ax.spines[&quot;top&quot;].set_visible(False) for col in [&quot;newFirst&quot;, &quot;newSecond&quot;, &quot;newTot&quot;]: if show_daily and col != &quot;newTot&quot;: # display daily data plot_column(df, col, ax, marker=&quot;.&quot;, linestyle=&quot;None&quot;) if show_rolling: # display 7 day averages plot_column(df, col, ax, forecast_date=forecast_date, rolling=True) ax.set_ylim(0, df[&quot;newTot&quot;].max() * 1.1) ax.set_xlabel(&quot;&quot;) ax.set_ylabel(&quot;Daily New Doses [millions]&quot;) ax.legend(loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5)) if title is not None: ax.set_title(title, fontsize=18) plot_daily_doses(df, title=f&quot;Daily Vaccinations up to {df.index.date.max()}&quot;) . Between January and until early February the vaccine doses administered per day steadily increased from around 0.25 million to 0.45 million on average. After that date rates plateaued and temporarily fell, mostly due to challenges and fluctuations in securing vaccine supplies. Late March saw the highest rates, peaking at 0.6 million doses per day in the week starting 14th March, though this then fell again, particularly over the Easter weekend (2nd-5th April), before recovering more recently. Supply difficulties are currently expected to continue in coming months. As well as fluctuations in the total, we can see that the number of second doses administered has been steadily increasing, and overtook the rate of new first doses in early April, with relatively few new first doses being given since then. More on this later. . If you look closely at the actual daily numbers of 1st and 2nd doses (the markers rather than the rolling average lines), it seems like they vary up and down with a period of 7 days. We can make this clearer by plotting the number of doses delivered on each weekday. . The day of the week for all the dates in our data can be determined using the pandas function day_name. Then we make use of the boxplot function in the seaborn plotting library to show the distribution in doses delivered on each weekday: . df[&quot;weekday&quot;] = df.index.day_name() plt.figure(figsize=figsize) sns.boxplot( x=&quot;weekday&quot;, y=&quot;newTot&quot;, data=df, order=[ &quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;, &quot;Sunday&quot;, ], ) plt.xlabel(&quot;&quot;) plt.ylim([0, plt.ylim()[1]]) plt.ylabel(&quot;Daily Doses (1st + 2nd) [millions]&quot;) . Text(0, 0.5, &#39;Daily Doses (1st + 2nd) [millions]&#39;) . More doses are administered Wednesday to Saturday than Sunday to Tuesday, with on average half as many (0.26 million) delivered on Sunday than on Saturday (0.53 million). Although I could not find an official explanation for this, one factor is likely related to the capacity and opening hours of vaccination centres and GP surgeries, which are more likely to be closed on Sundays for example. Other statistics have similar trends, such as fewer Covid-19 deaths being recorded at weekends due to reporting delays. . When are Second Doses Being Given? . A high-profile decision announced by the UK Joint Committee on Vaccination and Immunisation (JCVI) on the 30th December 2020, was to delay second doses by as long as possible, up to 12 weeks instead of the originally recommended 3-4 weeks. The decision was taken to be able to protect as many people as possible with a single dose of the vaccine within the first few months of the year, but was fairly controversial at the time due to clinical trials using shorter gaps between doses. . The delay between first and second doses will be an important component in our forecasts, so let&#39;s calculate when second doses are actually being given in practice (more precisely than &quot;up to 12 weeks&quot;). We can estimate this by determining the number of days until the total number of second doses (cumSecond) matches the total number of first doses (cumFirst) on a given date. . The for loop below calculates the delay for all dates in the past, and then prints the most recent delay: . for date_2nd, row in df.sort_index(ascending=False).iterrows(): if row[&quot;cumSecond&quot;] &gt;= df[&quot;cumFirst&quot;].min(): # find the last date where number of 1st doses is less than # or equal to number of 2nd doses on current row date_1st = df[df[&quot;cumFirst&quot;] &lt;= row[&quot;cumSecond&quot;]].index.max() # calculate how many days it was until 2nd doses were given delay = (date_2nd - date_1st).days df.loc[date_2nd, &quot;delaySecond&quot;] = delay else: break avg_second_delay = df[&quot;delaySecond&quot;][-1] print( &quot;1st doses from&quot;, (df.index[-1] - timedelta(days=avg_second_delay)).date(), &quot;were completed&quot;, avg_second_delay, &quot;days later, on&quot;, df.index[-1].date(), ) . 1st doses from 2021-02-15 were completed 77.0 days later, on 2021-05-03 . Currently, second doses are lagging first doses by 77 days (eleven weeks). In the history of the data so far, the delay has varied between 71 and 77 days: . df[&quot;delaySecond&quot;].min(), df[&quot;delaySecond&quot;].max() . (71.0, 77.0) . With the gap increasing from 72 to 75 days over the Easter weekend (2nd-5th April) where fewer doses (of any type) were administered than normal, and slowly drifting towards longer delays since then: . fig, ax = plt.subplots(1, 1, figsize=figsize) df.loc[df.index &gt;= datetime(2021, 3, 21), &quot;delaySecond&quot;].plot(ax=ax) ax.set_xlabel(&quot;&quot;) ax.set_ylabel(&quot;2nd Dose Delay [days]&quot;) . Text(0, 0.5, &#39;2nd Dose Delay [days]&#39;) . The second dose delay values we calculate here are averages for the whole country. In reality, the actual gap between doses for each individual will vary depending on the availability of appointments and vaccination strategy in their area. . Forecasting Future Vaccinations . Now we have a good idea about what&#39;s happened so far, but knowing that how can we forecast when the UK might fully vaccinate the adult population? At a high level, there will be two components to our forecasts: estimates for the total number of doses that will be administered on each future date, and of how those doses will be divided between first doses and second doses. We won&#39;t specify the exact form of our total daily doses estimation for now, except saying that it will be a function depending on two arguments - the date we want an estimate for, and the previous vaccination data. This general form will allow us to quickly try a few different forecasting approaches later. . For the second part (deciding which of the available vaccines will be given as first or second doses) we will make a number of assumptions that apply to all the forecasts we generate, as follows: . Everyone is given two doses of a vaccine. | Second doses are given avg_second_delay days after first doses (as calculated above). This means we expect the cumulative total of second doses today to match the cumulative total of first doses avg_second_delay days ago. Or equivalently, the number of new second doses required today is the difference between the current cumulative total of second doses and the cumulative total of first doses avg_second_delay days ago. | Second doses are given priority over new first doses. New first doses will only be given if the total number of doses available on a given day is more than the number of people requiring a second dose. | Once there is capacity to do so, second doses can be given sooner than avg_second_delay days later. Current UK guidance is that second doses of the Pfizer vaccine can be administered after 3 weeks, or after 4 weeks for the AstraZeneca vaccine, and up to 12 weeks later for both. | 100% of the UK adult population will be vaccinated. | . These assumptions generally seem reasonable, but there are limitations with all of them. It&#39;s likely that booster doses of Covid-19 vaccines will be needed, so ultimately people may receive more than two doses (so we&#39;re predicting only the completion of the first phase of the UK vaccination programme). Alternatively, single-dose vaccines such as the Johnson &amp; Johnson vaccine may start to be administered. New vaccine types coming online may also invalidate our third assumption above, as doses of a new vaccine can only be given as first doses initially - they can&#39;t be prioritised for second doses straight away like other vaccines (unless people begin to be given two doses of different vaccine types). Although it seems reasonable to give second doses earlier once there is spare capacity to do so, we don&#39;t know whether this will be the case and it may be that spare capacity is re-directed elsewhere. We&#39;ve also seen that the delay between doses has varied between 71 and 77 days so far, but we initially keep it fixed at the most recent value of 77 days. Finally, vaccine uptake in the over 50s has been close to 95% overall, but it&#39;s not 100% and has been much lower in certain sub-groups as discussed earlier. It&#39;s also possible uptake will decrease further in younger age groups. We should keep all these limitations in mind when interpreting our results. . The forecast_vaccines function below encodes everything described above. We give the vaccination data so far (as the input df), add new rows for future dates up to a specified end_date, determine how many doses will be administered on each date using a function doses_fn (that we haven&#39;t defined yet), and distributes those doses according to our assumptions. In many places the timedelta function from the python datetime library is used to get data from a number of days before the forecast date. . def forecast_vaccines( df, avg_second_delay, doses_fn, uk_pop=priority_totals[&quot;All Adults&quot;], end_date=datetime(2021, 12, 1), min_second_delay=28, ): &quot;&quot;&quot; Forecast future vaccine doses. df: DataFrame of actual vaccine data. avg_second_delay: Days after 1st dose that 2nd doses will be given. doses_fn: Function to calculate number of doses administered each day. Takes 2 arguments - df and a date. uk_pop: Total population to be vaccinated. end_date: Forecast until this date. min_second_delay: If there is spare capacity, allow second doses to be given earlier than avg_second_delay, down to this number of days. &quot;&quot;&quot; # extend our time series index to the future first_data = df.index.min() last_data = df.index.max() if end_date &lt; last_data: raise ValueError( f&quot;end_date ({end_date}) should be after the last date in df {last_data}&quot; ) future_dates = pd.date_range(last_data, end_date, closed=&quot;right&quot;) df = df.append(pd.DataFrame(index=future_dates)) df.sort_index(inplace=True) for d in future_dates: if d - timedelta(days=avg_second_delay) &lt; first_data: # no 1st dose data avg_second_delay ago, assume no 2nd doses required pending_2nd_doses = 0 else: # 2nd doses needed is difference between 2nd doses so far and 1st # doses avg_second_delay days ago (who now require 2nd dose) dose2_req = df.loc[d - timedelta(days=avg_second_delay), &quot;cumFirst&quot;] dose2_sofar = df.loc[d - timedelta(days=1), &quot;cumSecond&quot;] pending_2nd_doses = max([0, dose2_req - dose2_sofar]) # use forecasting function to determine number of doses available today total_doses_today = doses_fn(df, d) # don&#39;t vaccinate more than the total population (with 2 doses) if total_doses_today + df.loc[d - timedelta(days=1), &quot;cumTot&quot;] &gt; 2 * uk_pop: total_doses_today = 2 * uk_pop - df.loc[d - timedelta(days=1), &quot;cumTot&quot;] # give all 2nd doses required (up to limit of total doses available) dose2_today = min(pending_2nd_doses, total_doses_today) # remaining vaccines given as new 1st doses dose1_remaining = uk_pop - df.loc[d - timedelta(days=1), &quot;cumFirst&quot;] dose1_today = min(total_doses_today - dose2_today, dose1_remaining) # if there are spare doses, try giving 2nd doses earlier than usual if dose1_today + dose2_today &lt; total_doses_today: dose2_poss_early = df.loc[d - timedelta(days=min_second_delay), &quot;cumFirst&quot;] dose2_today = min(total_doses_today - dose1_today, dose2_poss_early) # save today&#39;s values df.loc[d, &quot;newSecond&quot;] = dose2_today df.loc[d, &quot;cumSecond&quot;] = ( df.loc[d - timedelta(days=1), &quot;cumSecond&quot;] + dose2_today ) df.loc[d, &quot;newFirst&quot;] = dose1_today df.loc[d, &quot;cumFirst&quot;] = df.loc[d - timedelta(days=1), &quot;cumFirst&quot;] + dose1_today df.loc[d, &quot;newTot&quot;] = dose1_today + dose2_today df.loc[d, &quot;cumTot&quot;] = df.loc[d, &quot;cumFirst&quot;] + df.loc[d, &quot;cumSecond&quot;] return df . The final piece of estimating the number of doses that will be available in the future is also where we have the least certainty. The scale and speed of the Covid-19 vaccine roll-out worldwide is unprecedented and there have been challenges, such as with lower production than expected in supply chains, and other delays such as the roll-out of the AstraZeneca vaccine being paused in some countries to investigate side-effects. . Given these circumstances it&#39;s possible future vaccine supply in the UK could be quite different to past supply. But as a starting point let&#39;s consider a simpler question - when will the UK population be vaccinated if doses continue to be given at the same rate as the last two weeks? In the last two weeks, vaccines have been given at a rate of: . n_days = 14 mean_doses_this_week = df[&quot;newTot&quot;].tail(n_days).mean() print( &quot;Between&quot;, df.index[-n_days].date(), &quot;and&quot;, df.index[-1].date(), f&quot;there was a mean of {mean_doses_this_week:.2f}&quot;, &quot;million doses given per day.&quot;, ) . Between 2021-04-20 and 2021-05-03 there was a mean of 0.49 million doses given per day. . Now all we need to run our forecast is to create the function to calculate how many doses are available each day. In this case it can be a simple function (const_doses below) that always returns the 0.49 million average daily doses calculated above. Even though we don&#39;t use them here, the function must have two inputs (df and date) to be compatible with our general forecast_vaccines function created above. Let&#39;s wrap all this into a function forecast_const we can easily use to run forecasts with other constant daily dose totals, then run it on our last week average: . def forecast_const( df, avg_second_delay, daily_doses, uk_pop=priority_totals[&quot;All Adults&quot;], end_date=datetime(2021, 10, 1), min_second_delay=28, ): &quot;&quot;&quot; Forecast vaccines assumming &#39;daily_doses&#39; doses are given per day. &quot;&quot;&quot; def const_doses(df, date): return daily_doses df_forecast = forecast_vaccines( df, avg_second_delay, doses_fn=const_doses, uk_pop=uk_pop, end_date=end_date, min_second_delay=min_second_delay, ) return df_forecast df_forecast = forecast_const(df, avg_second_delay, mean_doses_this_week) . We can re-use our plotting functions to examine our forecast. Let&#39;s start with the daily number of 1st and 2nd doses given per day: . last_data = df.index.max() plot_daily_doses( df_forecast, forecast_date=last_data, show_daily=False, title=f&quot;Daily Vaccinations Forecast (as of {df.index.date.max()})&quot;, ) . As expected, the total number of (1st + 2nd) doses given per day remains constant throughout our simple forecast. In the 1st and 2nd doses we see an interesting pattern where every couple of months the doses being administered swap between being mostly first doses or mostly second doses. In fact, the period over which this happens is approximately the 77 day gap between first and second doses we have assumed. At the start of the year (almost) the whole vaccine capacity was dedicated to giving new first doses, and then starting from late March all those people must be given second doses, meaning there is little spare capacity for new first doses. Our forecast predicts we will need to go through this cycle two times to vaccinate the whole population with two doses. . Now let&#39;s look at when we predict each sub-group of the population will be vaccinated: . plot_cumulative_doses( df_forecast, forecast_date=last_data, figsize=(15, 8), title=f&quot;UK Vaccination Forecast (as of {df.index.date.max()})&quot;, ) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;UK Vaccination Forecast (as of 2021-05-03)&#39;}, ylabel=&#39;Number of Doses [millions]&#39;&gt; . The current phase of mostly second dose vaccines being given leads to 32 million people (the population of groups 1-9) being fully vaccinated by late June in our forecast. The rest of the adult population is given a first dose of the vaccine by 19th July, and then is fully vaccinated on 26th August. Note that this gap is only 38 days (less than the 77 days between doses we use initially), as we allow second doses to be given earlier once there is spare capacity to do so. . Alternative Forecasts . Our forecast assumed doses will continue to be given at the rate from the the last two weeks, but looking back at the data we can see that this is higher than at most other times so far this year. Although it&#39;s possible the number of daily doses increases further in the future, we could consider that to be fairly optimistic. It may seem unlikely that vaccination rates fall to the levels seen at the start of the year, but there are reasons this could be the case. In particular, near the end of the vaccination programme demand may be lower in the remaining population, which was the case in Israel earlier this year for example. So what if we ask a more pessimistic question - when will the population be vaccinated with two doses if vaccines are administered at the same rate as the whole year so far? Or, even better, what is the range of likely completion dates? . To answer this, we&#39;ll create a new doses_fn (rnd_doses below) where the total number of doses given on any date in the future is the same as the number given on a day in the past, with that day selected randomly. We also limit the date in the past to have the same weekday as the forecast date, to take into account the differences during the week seen earlier (with more doses given Thursday-Saturday). Because the number of doses is chosen randomly, each time the forecast is run we&#39;ll get a different result. By running the forecast many times we can get an estimate of the uncertainty in the completion dates. . This is done using forecast_rnd function below, which returns a list of 200 forecasts: . def forecast_rnd( df, avg_second_delay, uk_pop=priority_totals[&quot;All Adults&quot;], end_date=datetime(2021, 10, 1), min_second_delay=28, rng=np.random.default_rng(seed=123), n_forecasts=100, ): &quot;&quot;&quot; Run n_forecasts random forecasts, using the random number generator rng to randomly choose the number of doses today to be the same as a date in the past on the same weekday. &quot;&quot;&quot; def rnd_doses(df, date): doses = rng.choice( df.loc[df.index.dayofweek == date.dayofweek, &quot;newTot&quot;].dropna() ) return doses forecasts = [ forecast_vaccines( df, avg_second_delay, doses_fn=rnd_doses, uk_pop=uk_pop, end_date=end_date, min_second_delay=min_second_delay, ) for _ in range(n_forecasts) ] return forecasts n_forecasts = 200 rnd_forecasts = forecast_rnd(df, avg_second_delay, n_forecasts=n_forecasts) . If we plot the number of daily doses in each of the 200 forecasts we get a spread of values centred around the average rate for the year, as expected: . plt.figure(figsize=(15, 5)) for fcast in rnd_forecasts: plt.plot( fcast[&quot;newTot&quot;].rolling(window=7).mean(), alpha=10 / n_forecasts, color=&quot;k&quot; ) plt.ylabel(&quot;Doses [millions]&quot;) plt.title(&quot;Daily New Doses (7 day Average)&quot;) . Text(0.5, 1.0, &#39;Daily New Doses (7 day Average)&#39;) . Rather than plotting each of the 200 individual forecasts, let&#39;s calculate 50% (median), 2.5% and 97.5% quantiles of the forecasts, specifically of the cumulative total of first and second doses. 95% of the forecast values lie between the 2.5% and 97.5% quantiles, so we can be confident the future data will be in that range (but only under our assumption that vaccines are given at the same rate as the year so far). . To do that we can combine all the data for one column into a single data frame with the pd.concat function, then calculate the quantiles with the pandas quantiles function: . quantiles = [0.025, 0.5, 0.975] q_forecasts = {} for i, col in enumerate([&quot;cumFirst&quot;, &quot;cumSecond&quot;]): rnd_col = pd.concat( [fcast[col].rename(f&quot;forecast{i}&quot;) for fcast in rnd_forecasts], axis=1, names=[0, 1], ) q_forecasts[col] = rnd_col.quantile(quantiles, axis=1) . Now let&#39;s plot the median of our forecasts, create a shaded range between the 2.5% and 97.5% quantiles to show the uncertainty (with the matplotlib function fill_between), and also display our original &quot;last week&quot; forecast to compare: . ax = plot_cumulative_doses( pd.DataFrame( { &quot;cumFirst&quot;: q_forecasts[&quot;cumFirst&quot;].loc[0.5], &quot;cumSecond&quot;: q_forecasts[&quot;cumSecond&quot;].loc[0.5], } ), forecast_date=last_data, figsize=(15, 8), ) q_start = 0.025 q_end = 0.975 alpha = 0.25 for col in [&quot;cumFirst&quot;, &quot;cumSecond&quot;]: # add shaded region between q_start and q_end quantiles ax.fill_between( q_forecasts[col].loc[q_start].index, q_forecasts[col].loc[q_start], q_forecasts[col].loc[q_end], color=col_format[col][&quot;color&quot;], alpha=alpha, label=f&quot;95% interval {col_format[col][&#39;label&#39;]}&quot;, ) # also show previous (last week average) forecast, to compare df_forecast[col].plot( linestyle=&quot;None&quot;, marker=&quot;.&quot;, markersize=4, color=col_format[col][&quot;color&quot;], label=f&quot;Last 2 weeks, {col_format[col][&#39;label&#39;]}&quot;, ax=ax, ) ax.legend() . &lt;matplotlib.legend.Legend at 0x7fe930c85c50&gt; . Under these assumptions, the population is fully vaccinated with two doses a couple of weeks later in mid-September rather than late-August. Note that our original forecast using the rate from the last 2 weeks lies outside of the uncertainty band for our new forecast. In other words, the population will only be vaccinated by the end of August if vaccines continue to be given at their current high rate without reverting back to the lower rates seen earlier in the year. . If we implemented a more sophisticated forecasting algorithm, such as an autoregressive or Bayesian structural time series model, we may be able to get improved estimates that incorporate the likelihood of both our &quot;optimistic&quot; and &quot;pessimistic&quot; cases. . Government Targets and SAGE Vaccine Roll-out Estimates . The government has the target of all UK adults being offered a first dose of the vaccine by the end of July. This is roughly in line with both of our forecasting approaches, with our optimistic forecast being a bit earlier in mid-July and the pessimistic forecast early-August. At the end of March, the Scientific Advisory Group for Emergencies (SAGE) also produced a report including the following two scenarios for the vaccine roll-out in England: . Fast scenario: 0.39m doses per day in England until week commencing 26th July and 0.29m per day thereafter | Slow scenario: 0.36m doses per day in England until week commencing 26th July and 0.29m per day thereafter | . Approximately scaling these up for the whole UK (56 million population England, 67 million population UK) gives: . Fast scenario: 0.46m doses per day until week commencing 26th July and 0.34m per day thereafter | Slow scenario: 0.43m doses per day until week commencing 26th July and 0.34m per day thereafter | . The SAGE forecasts use an 11-week (77 day) period between doses, which is also in line with what we derived ourselves from the data. . We can implement the assumptions from SAGE ourselves in our own forecasts just by creating a new doses_fn (sage_doses below) that returns a fixed number of doses depending on whether we&#39;re using the fast or slow strategy, and whether the date is after the 26th July or not: . def forecast_sage( df, avg_second_delay=77, scenario=&quot;fast&quot;, uk_pop=priority_totals[&quot;All Adults&quot;], end_date=datetime(2021, 10, 1), min_second_delay=77, ): &quot;&quot;&quot; Forecast vaccines for the SAGE &#39;fast&#39; and &#39;slow&#39; scenarios. &quot;&quot;&quot; if not (scenario == &quot;fast&quot; or scenario == &quot;slow&quot;): raise ValueError(&quot;scenario must be &#39;fast&#39; or &#39;slow&#39;&quot;) def sage_doses(df, date, scenario=scenario): if date &gt; datetime(2021, 7, 25): return 0.34 else: if scenario == &quot;fast&quot;: return 0.46 else: return 0.43 df_forecast = forecast_vaccines( df, avg_second_delay, doses_fn=sage_doses, uk_pop=uk_pop, end_date=end_date, min_second_delay=min_second_delay, ) return df_forecast . Then we can generate and plot the forecasts in the same way as we&#39;ve done before (displaying both the fast and slow strategies in the same figure): . df_sage_fast = forecast_sage( df, scenario=&quot;fast&quot;, ) df_sage_slow = forecast_sage( df, scenario=&quot;slow&quot;, ) ax = plot_cumulative_doses( df_sage_fast, forecast_date=last_data, figsize=(15, 8), title=f&quot;SAGE Scenarios&quot;, forecast_label=&quot;SAGE Fast&quot;, ) for col in [&quot;cumFirst&quot;, &quot;cumSecond&quot;]: ax.plot( df_sage_slow.loc[df_sage_slow.index &gt; last_data, col], color=col_format[col][&quot;color&quot;], linestyle=&quot;:&quot;, linewidth=2, label=f&quot;SAGE Slow {col_format[col][&#39;label&#39;]}&quot;, ) ax.plot( publish_date, publish_date_data[col], &quot;o&quot;, color=col_format[col][&quot;color&quot;], markersize=10, label=f&quot;{publish_date.strftime(&#39;%d %b&#39;)}, {col_format[col][&#39;label&#39;]}&quot;, ) ax.legend() . &lt;matplotlib.legend.Legend at 0x7fe9712404d0&gt; . The SAGE forecasts are similar to our more pessimistic forecasts, with the population fully vaccinated in mid-September, though the difference between their &quot;slow&quot; and &quot;fast&quot; strategies is smaller than the uncertainty we calculated. This gives us confidence that our forecasting approaches are reasonable, though it&#39;s interesting that SAGE does not expect the current rate of vaccinations to be maintained. It may be that they are aware of upcoming difficulties in vaccine supplies, or simply that taking a cautious approach is more appropriate for their purposes. . We&#39;ve also included markers in the figure above that show the current status on 16th May, the time of publication. The actual values look similar to our forecast values, but there have actually been more total doses (first and second combined) overall than we predicted, with 6.7 million doses since the 4th May. In comparison, our optimistic forecast using the average doses in the 2 weeks leading up to 4th May predicted 6.4 million doses, or 5.7 million in our pessimistic forecast using the average rate from the whole year so far (the SAGE scenarios are 5.6 million and 6.0 million). However, we have correctly predicted how many second doses would be given by this stage - 4.66 million compared to 4.65 million in our forecasts assuming an eleven week gap between doses. We look forward to seeing how this develops, and plan to do a story in the future to evaluate how well our forecasts performed! . Conclusion . In this Turing Data Story, we have used data collected from the UK government&#39;s Covid-19 data dashboard (using their API) to estimate when the UK may finish fully vaccinating its adult population. . With the demands and challenges of worldwide vaccine supply, it&#39;s difficult to accurately estimate how many vaccine doses will be available in the future, so we generated two sets of forecasts - a more optimistic forecast assuming that vaccines will continue to be given at an average of 0.49 million doses per day, which was the rate in the two weeks up to the 4th of May, and a more pessimistic forecast assuming vaccines are given at a rate consistent with the whole year so far. In our optimistic forecast the population is fully vaccinated on 26th August, or in our pessimistic forecasts a few weeks later in mid-September. Our pessimistic forecast is broadly in agreement with the assumptions in modelling by SAGE and with government targets. . As well as the uncertainty in future vaccine supply and demand, our forecasts made a number of simple assumptions about the vaccine roll-out which may not prove to be correct. We assumed 100% of the adult population will be vaccinated, for example, but we know take up rates are lower in certain sub-groups and it may be that take up will also be lower in younger people who are less at risk. In addition, we&#39;ve assumed second doses will continue to be given 11-weeks after first doses, and haven&#39;t considered differences between current and new vaccine types (which may have different dosing regimens) or regional differences. Finally, our forecasts assume constant vaccination rates, but a model incorporating the possibility of upward or downward trends may be more accurate. All these combined mean the range of feasible completion dates is wider than we&#39;ve calculated. . Nevertheless, we can be hopeful that the majority of the population will be vaccinated by early autumn, and that life in the UK will be much less impacted by Covid-19 on a daily basis by that time, though it&#39;s difficult to say whether it will return back to what was considered to be &quot;normal&quot; before 2020! Finishing by the autumn would also be perfect timing for the anticipated start of giving further booster doses to the most vulnerable in time for winter. . Worldwide, Covid-19 case rates are sadly still high and vaccination rates low in many countries at the time of writing. This excellent dashboard from Our World in Data shows vaccination progress worldwide, including links to data sources that (with some pre-processing) could also be fed into the forecasting functions developed here. For European countries, the European Centre for Disease Prevention and Control has a combined dataset available. Do feel free to experiment with the code from this story; ideas could include looking at the forecast results for different dates in the past (by changing run_as_date), using data from other countries, or trying different forecasting algorithms, for example. Thanks for reading! .",
            "url": "https://alan-turing-institute.github.io/TuringDataStories-fastpages/2021/05/19/COVID-19-Vaccine-Forecasting.html",
            "relUrl": "/2021/05/19/COVID-19-Vaccine-Forecasting.html",
            "date": " • May 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Modelling Mail-In Votes In the 2020 US Election",
            "content": "Author . Eric Daub, Alan Turing Institute, GitHub: @edaub. | . Other Contributors . Camila Rangel Smith, Alan Turing Institute, GitHub: @crangelsmith. Twitter: @CamilaRangelS. | Martin O&#39;Reilly, Alan Turing Institute, Github: @martintoreilly. | . Reviewers . Sam Van Stroud, University College London, GitHub: @samvanstroud | Kevin Xu, Civil Service Fast Stream, GitHub: @kevinxufs | . Introduction . The Covid-19 Pandemic led to record numbers of mail-in votes in the 2020 United States Presidential Election. Because of the high volume of mail ballots, plus rules that prevented some states from counting these ballots before election day, the result of the election remained uncertain for a week, with periodic updates coming as ballots were tabulated and reported. . In particular, several states had very close races that had the potential to tip the election in favor of either candidate. The US elects the president using the Electoral College system, where every state has a fixed number of electoral votes depending on its population. These electoral votes determine the outcome, not the national popular vote. The states almost universally employ a &quot;winner-take-all&quot; model for allocating their electoral votes. Because of this, each year a few &quot;swing states&quot; have a large effect on the outcome of the election. For example, in 2000 the election came down to around a 500 vote margin in Florida (out of over 6 million ballots cast), despite the fact that Al Gore easily won the national popular vote. In 2020, a few states with very close races dominated the headlines for the week after the election, of which we will look at Pennsylvania, Arizona, and Georgia in this post. The final outcome of the election hung on the results from these states, and the slow drip feed of additional ballots being released left the public constantly checking the news for updates. . In this Turing Data Story, I examine a few ways to analyze the data updates coming from each state to predict the final outcome. This originated from some Slack discussions with Camila Rangel Smith and Martin O&#39;Reilly, whom I list above as contributors for this reason. In particular, our initial interest in this question centered around uncertainties in the analysis done by Camila and Martin, which I have carried out using Bayesian inference to quantify uncertainty and determine when we might have reasonably called each state for the eventual winner based on the data as it underwent regular updates. . Data . To create the models in the post, I use the NYT Election Data Scraper, which is an open source repository that collected data from the New York Times website every few minutes over the months following the election. We can use this data to explore how the results evolved over time following election day, and compare our results with how the news media reported these changes over time. . In particular, the data includes an estimate of the number of votes remaining, which is a crucial figure that we need in order to mathematically forecast the outcome. The New York Times bases their model for the votes remaining using turnout estimates from Edison Research, which is mentioned here. Based on this turnout estimate, combined with the updates of votes as they are counted, we can use this data to forecast the outcome. . To load this data into a Python session for analysis, I can use Pandas to simply load from the CSV version of the data directly from the URL, and extract the state that I wish to examine: . %matplotlib inline . import pandas import datetime def load_data(state, timestamp=None): &quot;&quot;&quot; Loads election data updates from CSV file as a pandas data frame Retrieves data from the live file on Github, which is loaded into a data frame before extracting the relevant state data. State must be a string, which will be searched in the &quot;state&quot; field of the data frame. Timestamp must be a datetime string. Optional, default is current date and time. Returns a data frame holding all updates from a particular state, prior to the given timestamp. The &quot;vote_differential&quot; field is turned into a signed margin that is positive for a Biden lead. Also adds columns for the number of votes for Biden and Trump. &quot;&quot;&quot; if timestamp is None: timestamp = datetime.datetime.now().strftime(&quot;%Y-%m-%dT%H:%M:%S&quot;) try: data = pandas.read_csv(&quot;https://alex.github.io/nyt-2020-election-scraper/battleground-state-changes.csv&quot;) except: data = pandas.read_csv(&quot;battleground-state-changes.csv&quot;) data.loc[data[&quot;leading_candidate_name&quot;] == &quot;Trump&quot;, &quot;vote_differential&quot;] = -data.loc[data[&quot;leading_candidate_name&quot;] == &quot;Trump&quot;, &quot;vote_differential&quot;] data[&quot;biden_votes&quot;] = None data.loc[data[&quot;leading_candidate_name&quot;] == &quot;Biden&quot;, &quot;biden_votes&quot;] = data.loc[data[&quot;leading_candidate_name&quot;] == &quot;Biden&quot;, &quot;leading_candidate_votes&quot;] data.loc[data[&quot;trailing_candidate_name&quot;] == &quot;Biden&quot;, &quot;biden_votes&quot;] = data.loc[data[&quot;trailing_candidate_name&quot;] == &quot;Biden&quot;, &quot;trailing_candidate_votes&quot;] data[&quot;trump_votes&quot;] = None data.loc[data[&quot;leading_candidate_name&quot;] == &quot;Trump&quot;, &quot;trump_votes&quot;] = data.loc[data[&quot;leading_candidate_name&quot;] == &quot;Trump&quot;, &quot;leading_candidate_votes&quot;] data.loc[data[&quot;trailing_candidate_name&quot;] == &quot;Trump&quot;, &quot;trump_votes&quot;] = data.loc[data[&quot;trailing_candidate_name&quot;] == &quot;Trump&quot;, &quot;trailing_candidate_votes&quot;] data[&quot;timestamp&quot;] = pandas.to_datetime(data[&quot;timestamp&quot;]) data = data[data[&quot;timestamp&quot;] &lt; pandas.to_datetime(timestamp)] return data[data[&quot;state&quot;].str.contains(state)] . Note that rather than specifying the leading and trailing candidates, I instead just convert the vote differential into a margin that is positive if Biden is leading and negative if Trump is leading. I also add columns for the total number of votes for Biden and Trump, which I will use later. . For instance, if I would like to see the data for Georgia: . df = load_data(&quot;Georgia&quot;) df.head() . state timestamp leading_candidate_name trailing_candidate_name leading_candidate_votes trailing_candidate_votes vote_differential votes_remaining new_votes new_votes_relevant ... trailing_candidate_partition precincts_reporting precincts_total hurdle hurdle_change hurdle_mov_avg counties_partition total_votes_count biden_votes trump_votes . 112 Georgia (EV: 16) | 2020-12-07 20:17:22.360 | Biden | Trump | 2473633 | 2461854 | 11779 | -2224 | -8 | 1 | ... | 75.000000 | 2655 | 2655 | 0.0 | 0.0 | 0.360367 | {} | 4997716 | 2473633 | 2461854 | . 113 Georgia (EV: 16) | 2020-12-05 00:26:57.152 | Biden | Trump | 2473707 | 2461779 | 11928 | -2232 | -842 | -858 | ... | 0.067599 | 2655 | 2655 | 0.0 | 0.0 | 0.368200 | {&#39;Appling&#39;: 49, &#39;Ben Hill&#39;: 4, &#39;Bleckley&#39;: 2, ... | 4997724 | 2473707 | 2461779 | . 114 Georgia (EV: 16) | 2020-11-20 22:11:31.395 | Biden | Trump | 2474507 | 2461837 | 12670 | -3074 | -954 | -917 | ... | 0.214831 | 2655 | 2655 | 0.0 | 0.0 | 0.364533 | {} | 4998566 | 2474507 | 2461837 | . 115 Georgia (EV: 16) | 2020-11-20 21:57:11.453 | Biden | Trump | 2475227 | 2462034 | 13193 | -4028 | -770 | -532 | ... | 0.855263 | 2655 | 2655 | 0.0 | 0.0 | 0.349900 | {} | 4999520 | 2475227 | 2462034 | . 116 Georgia (EV: 16) | 2020-11-20 21:51:12.226 | Biden | Trump | 2475304 | 2462489 | 12815 | -4798 | -1958 | -1851 | ... | 0.518098 | 2655 | 2655 | 0.0 | 0.0 | 0.356867 | {} | 5000290 | 2475304 | 2462489 | . 5 rows × 22 columns . The data contains a timestamp, the number of votes for each candidate, the margin, and an estimate of the number of votes remaining. Note that because the number of votes remaining is just an estimate, it can eventually become negative if the eventual number of ballots exceeds the estimate used to forecast the number of remaining votes. We can also have situations where the number of votes is corrected downwards, so we will need to take care that these do not trip up our model. . When the data is captured in this way, we can see how the vote margin evolves over time as new ballots are counted. For example, we can look at the data for all states up to midnight on 5 November to see the evolution of the race: . import matplotlib.pyplot as plt state_list = [&quot;Pennsylvania&quot;, &quot;Georgia&quot;, &quot;Arizona&quot;] timestamp_list = [&quot;2020-11-05T00:00:00&quot;]*3 iter_vals = list(zip(state_list, timestamp_list)) def plot_data(state, timestamp=None): &quot;Plot the election data for a given state up through a given time&quot; df = load_data(state, timestamp) plt.figure() plt.plot(df[&quot;votes_remaining&quot;], df[&quot;vote_differential&quot;], &quot;o&quot;) plt.xlabel(&quot;Votes remaining&quot;) plt.ylabel(&quot;Biden margin&quot;) plt.title(&quot;{} Vote Updates through {}&quot;.format(state, timestamp)) for (state, tstamp) in iter_vals: plot_data(state, tstamp) . Note that the trend shows that Biden is catching up as more votes are counted in both Georgia and Pennsylvania, while Trump is catching up in Arizona. The trend is fairly linear. Thus, one might first consider doing a simple regression to estimate the final margin. . Linear Regression Analysis . As a first pass at building a model, I can do a simple regression analysis. A linear regression model will have two parameters that are fit: the slope will be related to the fraction of the outstanding votes that are for Biden, and the intercept, which will indicate the final margin when there are no votes remaining. (This is the initial analysis that was done by Camila for Pennsylvania and Martin for Arizona.) . import numpy as np def linear_regression(state, timestamp=None): &quot;&quot;&quot; Fit a linear regression model to the election updates Fits a line to the data updates for a given state and a given timestamp. Plots the data and returns the fitting parameters (slope, intercept) as a numpy array. &quot;&quot;&quot; plot_data(state, timestamp) df = load_data(state, timestamp) coeffs = np.polyfit(df[&quot;votes_remaining&quot;], df[&quot;vote_differential&quot;], 1) plotvals = np.linspace(0, df[&quot;votes_remaining&quot;].iloc[-1]) plt.plot(plotvals, coeffs[0]*plotvals + coeffs[1]) return coeffs for (state, tstamp) in iter_vals: coeffs = linear_regression(state, tstamp) print(&quot;Predicted margin for {} as of {}: {}&quot;.format(state, tstamp, coeffs[1])) . Predicted margin for Pennsylvania as of 2020-11-05T00:00:00: 139881.65460479335 Predicted margin for Georgia as of 2020-11-05T00:00:00: 11362.338299788926 Predicted margin for Arizona as of 2020-11-05T00:00:00: -87876.90143040325 . Note that at this point, the linear regression predicts a margin in Pennsylvania and Arizona that are quite different from the final margin. Georgia appears to be very close to the final margin. However, Arizona seems to have outlier points that muddles this analysis (which was first noted by Martin). Thus, while these models are useful starting points, they do not appear to be particularly robust and are somewhat dependent on when you choose to fit the data. . def get_margin(state, timestamp=None): &quot;Extract margin for a state at a given time&quot; df = load_data(state, timestamp) return df[&quot;vote_differential&quot;].iloc[0] for state in state_list: print(&quot;Current margin in {}: {}&quot;.format(state, get_margin(state))) . Current margin in Pennsylvania: 81660 Current margin in Georgia: 11779 Current margin in Arizona: 10457 . However, one thing to note about this is that even though the trends point clearly in favor of Biden in this analysis, we do not have a good idea of the uncertainties. Without an uncertainty, we cannot robustly evaluate if the model is making good predictions. How might we develop a model that explicitly captures this uncertainty? And given such a model, when can we be confident that a candidate has won the state, and how does it align with the narrative from the news media? The following describes one approach for doing so. . Modelling Uncertainty in the Votes . To address this shortcoming, we turn to Bayesian Inference. Bayesian statisticians think of model parameters not as a single number, but rather probability distributions -- in this way, we can get a sense of the range of values that the model thinks are consistent with the data. . Model Structure . As noted above, the regression model has two different parameters: the slope (related to the fraction of votes that are cast for Biden), and the intercept (which is essentially the prediction of the final margin). Note that while the linear regression fit these two things simultaneously, there is no reason why I had to let the final margin be a &quot;free&quot; parameter that was adjusted in the fitting: I could have instead just fit a single parameter for the slope (for instance, simply using the fraction of mail ballots cast thus far for one of the candidates), and then used that estimate to project the votes remaining in order to extrapolate and obtain an estimate of the final margin. . We would like to adjust our model to account for uncertainty in both of these pieces of the model. The main source of uncertainty in the vote probability is related to the fact that not all voters are identical -- the regression model assumes that this is the case, and the main challenge in building a more complex model is to relax this constraint while still ensuring that the model is simple enough that we can reliably fit it with the available data. For this, I will propose what is known as a hierarchical model, which is a common way of adding more complexity to a model in Bayesian inference. However, this is not the only way to do this, and there are certainly other methods based on Bayesian inference that would be able to account for this type of uncertainty. . There is also some uncertainty to account for in projecting the remaining votes, but it turns out that this is much smaller than the uncertainty in the vote probability. The way that I handle this type of uncertainty is a fairly standard problem in Bayesian inference, so I will focus most of my attention here on how to make a model that does not treat all voters identically, as this is the principal source of uncertainty. The following sections outline how to build such a model, fit its parameters, and then project the election outcome once it has been fit. . Bayesian Model of the Vote Probability . Bayesian inference tends to think of probability distributions as reflecting statements about our beliefs. Formally, I need to state my initial beliefs before I see any data, and then I can use that data to update my knowledge. This previous belief is known as a prior in Bayesian inference, and the updated beliefs once I look at the data is known as the posterior. . Bayesian Inference . Bayesian inference involves taking our previous beliefs about a system, described by a probability distribution of reasonable values we expect a particular parameter to take, and then using the data to update those beliefs about the distribution that we expect that parameter to take by computing the posterior. A key concept in Bayesian statistics is the idea of a conditional probability, written $p(A|B)$, which means the probability of $A$ given that we already know $B$ (or conditioned on $B$). Inference allows us to update our beliefs (or in other words condition them on something we have observed) by applying Bayes&#39; rule: . $$ p( theta|y) = frac{p(y| theta)p( theta)}{p(y)} $$ . Here, $p( theta)$ is the prior distribution (which we will specify before looking at the data), $p(y| theta)$ is the likelihood (the probability that we would have gotten the data conditioned on a particular value of $ theta$), and $p(y)$ is known as the evidence (the probability of getting that particular observation over all possible outcomes of the experiment). Note that we have two standard probability distributions (the prior and the evidence), and two conditional probabilities (the likelihood and the posterior). The one we really care about is the posterior (our belief in the model parameters conditioned on the data, since the model parameters are uncertain but the data is known), but in practice it is much easier to compute the likelihood; Bayes&#39; rule tells us how these two are connected and makes these kind of computations more practical. . In practice, for most models one cannot compute the evidence very easily, so instead of computing the posterior directly, one draws samples from it. A common technique for this is Markov Chain Monte Carlo (MCMC) sampling. A number of software libraries have been written in recent years to make carrying out this sampling straightforward using what are known as probabilistic programming languages. These languages formally treat variables as probability distributions with priors and then draw samples from the posterior to allow the user to more easily perform inference. . Note that because I will ultimately draw samples from the posterior, rather than compute the probability density function itself, most of the plots in this story will not have meaningful vertical axis scales, as the scale will depend on the number of samples that were drawn. Thus in most cases I simply omit labels on the vertical axes, as these are implicitly the number of samples in each case. . A Hierarchical Bayesian Model For Voting . In the linear regression model, we effectively treat the vote probability as a single, unchanging value. This is the same as saying that every voter in our model is identical. Given the political polarization in the US, this is probably not a very good assumption. Although the data seems to strongly suggest that the mail-in votes are consistently in favor of one candidate, this is not the same as saying all voters are identical. In the following, I build a model to relax this assumption, using what is known as a hierarchical Bayesian model. . If the above model of assuming that every voter is identical is one extreme, then the other extreme is to assume that every voter is different and we would need to estimate hundreds of thousands to millions of parameters to fit our model. This is not exactly practical, so a hierarchical model posits a middle ground that the vote probability is itself drawn from a probability distribution. In this manner, we can use a probability distribution to describe the overall population of voters, and thus rather than fitting a parameter for every single voter, we just need to fit a few parameters to specify the distribution. . This structure is why hierarchical models are so powerful: probability distributions can be described with only a small number of parameters, so they are easy to fit, but we will see that it vastly expands the range of outcomes that the model is able to produce. This is in essence why hierarchical models can be so powerful -- they guard against overfitting by using only a few parameters, but they can still produce a large variety of outcomes. . In the following, we model the vote probability by assuming that each vote update has a single vote probability associated with it, and that vote probability is drawn from a beta distribution. A beta distribution is a distribution defined over the interval $[0,1]$ with two shape parameters $a$ and $b$ that lets us flexibly specify a wide range of outcomes. If $a$ and $b$ are less than 1, then the distribution is biased towards the extreme values of 0 or 1, while if they are greater than 1 then the distribution is biased towards 0.5. If $a &gt; b$, then the model is more biased towards 1, while if $b &gt; a$ then the model is biased towards 0. Thus we can specify a huge range of distributions with just two parameters. . from scipy.stats import beta def plot_beta(a, b): &quot;Plot the beta distribution for shape paramters (a, b)&quot; xvals = np.linspace(0, 1) plt.plot(xvals, beta.pdf(xvals, a=a, b=b)) plt.xlabel(&quot;Biden vote probability&quot;) plt.title(&quot;PDF for the Beta distribution with a = {}, b = {}&quot;.format(a, b)) plot_beta(8., 4.) . Thus, instead of estimating the vote probability, I instead need to estimate $a$ and $b$, which will tell us what I expect the distribution of the vote probability to be. Having multiple levels of distributions described by distributions like this are what give hierarchical models their name -- parameters are drawn from distributions, and this distribution must be described by some parameters. But because all parameters in Bayesian inference are probability distributions, these parameters are also distributions themselves, hence the model is hierarchical. . Since all parameters in a Bayesian model must have priors, our task is now to encode our prior beliefs about the vote probability distribution by setting prior distributions for $a$ and $b$. . Prior . The prior represents our previous beliefs about the value of the model parameters before we see any data, or in other words what we expect to be reasonable. If you are observing outcomes from flipping a coin, you might reasonably expect the probability of getting heads to be close to 0.5 in the absence of any data, and you might be unwilling to accept the idea of this probability of being 0 or 1. This is encoded in a probability distribution function, usually by using a particular distribution that is convenient given the constraints of the particular problem (for instance, the beta distribution described above is one that is often used to model priors where the parameter in question is a probability). . Often, in Bayesian inference one does not have strong feelings about what values they might expect for a parameter. In those cases, we might prefer to use something simple, what is known as an uninformative prior. These might be expressed as a statement like &quot;every value is equally probable&quot;. Or in this case we might assume that the prior for the vote probability should be peaked close to 0.5, and then taper off towards 0 and 1, with the argument that US presidential elections are usually decided by a few percentage points difference in the national popular vote. This might seem very reasonable on the surface, as America is pretty evenly divided between Democrats and Republicans. . However, mail in votes in practice can be extremely biased towards one party. Historically, a large majority of mail in ballots are Democratic, for a variety of reasons that are largely demographic. Trump also spent much of the campaign sowing doubt about mail-in ballots (telegraphing his post-election strategy of trying to throw them out in court), so his supporters may be much less likely to vote in this manner. However, there could also be a situation where the mail-in ballots fall heavily towards a Republican candidate (as we have seen already, more of the Arizona ballots tend to be in favor of Trump, which was due to late arriving ballots being from Republicans that registered to vote just before the election). Thus, based on this I would argue that what we actually want is a prior that is reasonably likely to include some extremes in the vote probability to ensure that our estimate of the final outcome prior to looking at the data doesn&#39;t exclude a significant swing. . This issue illustrates a challenge with Bayesian Hierarchical models -- when the parameter that I have some knowledge about is itself described by a distribution, the priors for the distribution parameters can be more difficult to specify. For this reason, modellers often go one level further and specify prior distributions on the parameters used to specify the priors on the model parameters, which are known as hyperpriors, and see how varying the priors changes the outcome of inference. I will not explore this level of Bayesian modelling, but in building this model I had to try a number of different choices for the priors before arriving at something that I thought accurately reflected my prior beliefs about the outcome. . In the priors that I finally settled on, I use a Lognormal distribution for my prior on $a$ and $b$. Lognormal distributions are frequently used as an alternative to a normal distribution in situations when the parameter in question must be positive (effectively, lognormal distributions can thought of as having the logarithm of the random variable in question following a normal distribution). I choose the parameters of the lognormal distributions for $a$ and $b$ to be slightly different such that the resulting distributions are more likely to lean democratically (as mail in votes are historically more democratic leaning), but still have a decent chance of producing extremes for Trump. I also choose the parameters such that I get a mix of values more biased towards the extremes as well as those biased towards values closer to 0.5. . Priors like the ones used here are often referred to as subjective priors. Depending on who you ask, this is either a strength or a weakness in Bayesian inference. On one hand, the results are somewhat dependent on my parameter choices for my priors, but on the other hand, nearly every scientific study reflects some of the biases of the researchers that carry out the work. Subjective priors have the benefit of explicitly codifying the assumptions I made about what I expect to be reasonable outcomes, at the cost of some potential subjectivity in the final results. In this era of significant divides across party lines, exposing this part of the model as inherently subjective may also help ensure that the model parameters are not chosen in a way that explicitly favors one party over another. . As we will see, using this prior allows for the possibility that there is a decent chance based on historical data that the mail votes are heavily in favor of one candidate. The parameters I choose give a slight edge to the candidate from the Democratic party, but the prior includes a very reasonable chance that the swings will be towards the Republican candidate, which should help ensure that the model is not perceived to be biased against either candidate. Here are some histograms showing single samples of the vote probability drawn from this prior, and an aggregate histogram of 100 samples: . from pymc3 import Lognormal def plot_prior_samples(n_samples): &quot;Plot a random draw of the vote probability from the prior&quot; a = Lognormal.dist(mu=0.4, sd=0.5).random(size=n_samples) b = Lognormal.dist(mu=0.2, sd=0.5).random(size=n_samples) x = np.linspace(0., 1.) plt.figure() plt.hist(beta.rvs(size=(1000, n_samples), a=a, b=b).flatten(), bins=100) plt.xlabel(&quot;Biden vote probability&quot;) plt.title(&quot;Prior Vote Probability Distribution using {} samples&quot;.format(i)) for i in [1, 1, 1, 100]: plot_prior_samples(i) . From these individual samples, as well as the aggregated histogram, we see that we get a range of outcomes, with a slight bias towards those that favor democrats. As we acquire enough data to reliably estimate the underlying distribution of the vote probabilites, we should see better estimates of the true distribution, which will eliminate more of the extremes and reduce the uncertainty in the final outcome. . Likelihood . Finally, we need to explicitly model the likelihood. The likelihood is how Bayesian models connect the parameters to the data -- the likelihood tells us how probable the data is conditioned on a particular choice of parameters. As discussed earlier, this is actually the opposite of what we want, as we are interested in the parameters conditioned on the data. However, Bayes&#39; rule helps us compute the quantity we are interested in as long as we are able to to compute the likelihood. . For this particular problem, we can fortunately compute the likelihood easily in closed form. When one flips a fair coin a number of times, the distribution of outcomes follows a binomial distribution. Thus, I can use a binomial likelihood to model the range of vote probabilites that might be consistent with the votes that were cast. This can be computed analytically, and most probabilistic programming languages have built-in capacity for computing likelihoods of this type (so that we don&#39;t have to worry about writing down the correct formula and implementing and testing the code to do this ourselves!). This is done automatically when using a probabilistic programming language by setting this particular variable to have a known value, which indicates to the probabilistic programming language that this variable is used to compute the likelihood. This means that I can focus on describing the model, rather than how to do the computation, which is one of the strengths of this type of modelling. . PyMC3 Implementation . Thus, I can now write down a model in a probabilistic programming language in order to draw samples from the posterior. There are a number of popular lanaguages for this -- here I use PyMC3 to implement my model. PyMC3 can easily handle all of the features I specified above (hierarchical structure, and a binomial likelihood), which is written out in the function below. . In coding up the model, I need to convert the election data into a series of &quot;trials&quot; consisting of a number of total votes cast and the corresponding number of votes for Biden. This is the way that the information needs to be captured to compute the binomial likelihood, which can then be used to draw samples from the posterior. Earlier, I noted that the data sometimes contains some inconsistencies (i.e. there are more votes cast for one candidate than the total number in that batch), so to protect against this I perform some checks for consistency and throw out any data that doesn&#39;t make sense (in particular, I want to be sure the total number of votes is non-negative and the number of votes cast for Biden is smaller than the total number of ballots in this particular batch). I do this using a Numpy logical array in the extract_vote_trials function, as otherwise some inconsistent data can trip up the PyMC computations. . Once we have the trials defined, we can specify our model using PyMC3, perform inference, and plot the results: . import pymc3 import logging logger = logging.getLogger(&quot;pymc3&quot;) logger.propagate = False logger.setLevel(logging.ERROR) def extract_vote_trials(state, timestamp=None): &quot;&quot;&quot; Convert vote data into a series of bernoulli trials. If no data is valid (i.e. check that all numbers are nonzero and the number of total ballots cast is larger than the number of votes for, then return a list of a single zero for each. Returns two lists of positive integers for the total number of votes and the votes for Biden &quot;&quot;&quot; df = load_data(state, timestamp) total_votes = np.diff(-df[&quot;biden_votes&quot;]) + np.diff(-df[&quot;trump_votes&quot;]) biden_votes = np.diff(-df[&quot;biden_votes&quot;]) # throw out combinations that don&#39;t make any sense using numpy # logical arrays -- note that multiplication of logical arrays # is equivalent to a logical &quot;and&quot;; # if none remain then just return (0,0) trials_to_keep = ((total_votes &gt; 0)*(biden_votes &gt;= 0)* (biden_votes &lt;= total_votes)) total_votes = total_votes[trials_to_keep] biden_votes = biden_votes[trials_to_keep] if len(total_votes) == 0: total_votes = [0] biden_votes = [0] return (np.array(total_votes, dtype=np.int64), np.array(biden_votes, dtype=np.int64)) def estimate_theta_hierarchical(state, timestamp=None): &quot;Estimate the vote probability distribution using a hierarchical model and MCMC sampling&quot; total_votes, biden_votes = extract_vote_trials(state, timestamp) num_trials = len(total_votes) # build model and draw MCMC samples with pymc3.Model() as model: a = pymc3.Lognormal(&quot;a&quot;, mu=0.4, sd=0.5) b = pymc3.Lognormal(&quot;b&quot;, mu=0.2, sd=0.5) theta = pymc3.Beta(&quot;theta&quot;, alpha=a, beta=b, shape=num_trials) obs = pymc3.Binomial(&quot;obs&quot;, p=theta, n=total_votes, observed=biden_votes, shape=num_trials) trace = pymc3.sample(1000, progressbar=False, return_inferencedata=False) return trace def plot_posterior(state, timestamp): &quot;Plot the posterior distribution of the vote probability&quot; trace = estimate_theta_hierarchical(state, timestamp) rvs_size = (100, len(trace[&quot;a&quot;])) plt.figure() plt.hist(beta.rvs(size=rvs_size, a=np.broadcast_to(trace[&quot;a&quot;], rvs_size), b=np.broadcast_to(trace[&quot;b&quot;], rvs_size)).flatten(), bins=100) plt.xlabel(&quot;Biden vote probability&quot;) plt.title(&quot;{} Vote Probability Posterior as of {}&quot;.format(state, timestamp)) for (state, tstamp) in iter_vals: plot_posterior(state, tstamp) . Once I draw MCMC samples for $a$ and $b$, I convert those samples into samples of $ theta$ to see our posterior estimate of the vote probability. . Looking at these plots, I see that the model is now much more varied in its estimates for the vote probability (note that this is the posterior for the distribution expected for the vote probability, rather than the explicit values of the vote probability itself). The mean is still where I expect it based on the linear regression analysis, but the distribution is much wider due the fact that occasionally votes come in from places that are not as heavily in favor of Biden (or Trump in the case of Arizona). This wider distribution of the vote probability is how I quantify the larger uncertainty in the election outcome. Using this distribution to forecast the remaining votes should considerably increase the spread of the predicted final margin and assure that it is not overconfident in the final result. . Predicting the Final Margin . Once I have samples from the vote probability, I need to simulate the remaining votes to predict the final outcome. This is known as estimating the posterior predictive distribution in Bayesian inference, when one uses the updated knowledge about one of the model parameters to predict something that was not used to fit the data. . What is a reasonable way to simulate the remaining votes? As one can see from the data, the votes come in a steady drip feed as ballots are counted. Thus, I can simulate this by sampling randomly, with replacement, from the data for the number of ballots cast in each update until I get to the number of votes remaining. I can then use the posterior samples of $a$ and $b$ to generate a distribution of vote probabilities, and then draw from the vote probabilites to forecast the outcome of each batch of votes using a binomial distribution. I repeat this process 10 times to ensure that the result isn&#39;t dependent on the particular realization of the drip feed simulation, and aggregate those samples to get the final estimate of the posterior predictive distribution. This should give a reasonable estimate of the final outcome based on the model. . from scipy.stats import binom def get_votes_remaining(state, timestamp=None): &quot;Extract remaining votes for a state at a given timestamp&quot; df = load_data(state, timestamp) return df[&quot;votes_remaining&quot;].iloc[0] def draw_random_vote_updates(state, timestamp): &quot;Draw a random set of simulated vote updates for the remaining votes&quot; n_remain = get_votes_remaining(state, timestamp) n, k = extract_vote_trials(state, timestamp) if np.all(n == 0): n = np.array([1000], dtype=np.int64) simulated_vote_updates = [] while np.sum(simulated_vote_updates) &lt;= n_remain: simulated_vote_updates.append(np.random.choice(n)) simulated_vote_updates[-1] = n_remain - np.sum(simulated_vote_updates[:-1]) assert np.sum(simulated_vote_updates) == n_remain return np.array(simulated_vote_updates, dtype=np.int64) def project_remaining_votes(trace, simulated_vote_updates): &quot;Project the remaining votes using MCMC samples of the vote probability distribution parameters&quot; assert np.all(trace[&quot;a&quot;] &gt;= 0.) assert np.all(trace[&quot;b&quot;] &gt;= 0.) rvs_size = (len(trace[&quot;a&quot;]), len(simulated_vote_updates)) return np.sum(binom.rvs(size=rvs_size, p=beta.rvs(size=rvs_size, a=np.broadcast_to(trace[&quot;a&quot;][:, np.newaxis], rvs_size), b=np.broadcast_to(trace[&quot;b&quot;][:, np.newaxis], rvs_size)), n=np.broadcast_to(simulated_vote_updates, rvs_size)), axis=-1) def predict_final_margin(trace, state, timestamp=None): &quot;&quot;&quot; Use posterior samples of the vote probability to predict the remaining votes. The remaining votes are split into batches by sampling from the previous votes until enough are accumulated. Then each batch is forecast using the posterior samples, and the total is summed. Returns a numpy array of samples of the final margin &quot;&quot;&quot; # simulate remaining votes n_trials = 10 predicted_margin = np.zeros((n_trials, len(trace[&quot;a&quot;]))) for i in range(n_trials): simulated_vote_updates = draw_random_vote_updates(state, timestamp) predicted_margin[i] = project_remaining_votes(trace, simulated_vote_updates) n_remain = get_votes_remaining(state, timestamp) margin = get_margin(state, timestamp) return margin - n_remain + 2*predicted_margin.flatten() def plot_predictions(state, timestamp): &quot;Plot the posterior predictive distribution for the given state and time&quot; trace = estimate_theta_hierarchical(state, timestamp) predicted_margin = predict_final_margin(trace, state, timestamp) plt.figure() plt.hist(predicted_margin, bins=100) plt.xlabel(&quot;Biden Margin&quot;) plt.title(&quot;{} final predicted margin as of {}&quot;.format(state, timestamp)) for (state, tstamp) in iter_vals: plot_predictions(state, tstamp) . As we can see from this, the model has fairly wide intervals surrounding the predicted final margin based on the original linear regression model. Interestingly, when I fit Georgia in this way, it looks much more likely that Trump would win through this point than the linear regression model would suggest, though the final margin found by the regression analysis is well within the error bounds suggested from the predictions. Arizona looks up for grabs, indicating that the outlier points were definitely biasing the regression analysis. Pennsylvania is much more firmly leaning towards Biden. We can look at the results again a day later to see how the race evolved: . for (state, tstamp) in zip(state_list, [&quot;2020-11-06T00:00:00&quot;]*3): plot_predictions(state, tstamp) . Clearly, Georgia has swung in Biden&#39;s favor over the course of the day. The mean final margin in Pennsylvania has not moved much, though the uncertainty has tightened up and made the result more likely for Biden. Arizona could still go either way. . Animating the Updates . Now that I have built a model, I can build an animation that shows the evolution of the predicted results as a function of time. This will show how the uncertainty shrinks over time as fewer votes remain. I check for results every 30 minutes for the 12 days from 4 November onward, and update the model when new ballots are found. I also compute a Biden win probability and show the mean margin $ pm$ 2 standard deviations to give an idea of the equivalent regression result and its uncertainty. Building the animation requires some more clever manipulation of Matplotlib objects, so I will not go into detail to describe exactly what the plotting code is doing here. It is based on the code in this example, so please look at that example for a description of how to animate histograms. . Note: Because new MCMC samples need to be drawn for each new update, creating this animation ends up being fairly expensive to run (this took several hours on my laptop). I speed things up by saving the current prediction each time the MCMC samples are drawn, so that if the previous iteration is the same we do not need to re-run the model. However, this is still fairly expensive, so don&#39;t try and run this unless you are willing to wait! . %%capture import matplotlib.path as path import matplotlib.patches as patches import matplotlib.text as text import matplotlib.animation as animation def load_previous_model(filename, initialize=False): &quot;&quot;&quot; Load the previous model from file Load results from the previous simulation from disk, including the total votes cast, votes for Biden, number of votes remaining, and the previous set of samples from the predictive distribution. If the file cannot be loaded, or if we pass `initialize=True`, returns `None` for all values. &quot;&quot;&quot; total_votes = None biden_votes = None n_remain = None preds = None if not initialize: try: model = np.load(filename) total_votes = model[&quot;total_votes&quot;] biden_votes = model[&quot;biden_votes&quot;] n_remain = int(model[&quot;n_remain&quot;]) preds = model[&quot;preds&quot;] except (KeyError, IOError): total_votes = None biden_votes = None n_remain = None preds = None return total_votes, biden_votes, n_remain, preds def fit_model(state, timestamp=None, initialize=False): &quot;&quot;&quot; Fit a model to predict the final margin for the given date/time. Each iteration is saved as a numpy file, and the next step checks for a model that matches the existing vote count before doing the expensive MCMC fitting Returns the simulated final margin samples at the given time &quot;&quot;&quot; filename = &quot;model.npz&quot; total_votes_prev, biden_votes_prev, n_remain_prev, preds_prev = load_previous_model(filename, initialize) total_votes, biden_votes = extract_vote_trials(state, timestamp) n_remain = get_votes_remaining(state, timestamp) if (np.array_equal(total_votes_prev, total_votes) and np.array_equal(biden_votes_prev, biden_votes) and n_remain_prev == n_remain): return preds_prev else: theta = estimate_theta_hierarchical(state, timestamp) preds = predict_final_margin(theta, state, timestamp) np.savez(filename, total_votes=total_votes, biden_votes=biden_votes, preds=preds, n_remain=n_remain) return preds def initialize_verts(bins): &quot;Initialize the patch corners for the animation&quot; # get the corners of the rectangles for the histogram left = bins[:-1] right = bins[1:] vals = np.zeros(len(left)) nrects = len(left) nverts = nrects * (1 + 3 + 1) verts = np.zeros((nverts, 2)) codes = np.full(nverts, path.Path.LINETO) codes[0::5] = path.Path.MOVETO codes[4::5] = path.Path.CLOSEPOLY verts[0::5, 0] = left verts[0::5, 1] = vals verts[1::5, 0] = left verts[1::5, 1] = vals verts[2::5, 0] = right verts[2::5, 1] = vals verts[3::5, 0] = right verts[3::5, 1] = vals return verts, codes def update_verts(preds, bins, verts): &quot;Update the verticies on the histogram patches for animation&quot; n, bins = np.histogram(preds, bins) verts[1::5, 1] = n verts[2::5, 1] = n return verts def animate(i, state, start_time, bins_t, verts_t, patch_t, bins_b, verts_b, patch_b, date_text, vote_text, mean_text, prob_text): &quot;Updates the histogram patches and text to make the animated histogram plot&quot; hours = i//2 minutes = 30*i % 60 timestamp = ((datetime.datetime.strptime(start_time, &quot;%Y-%m-%dT%H:%M:%S&quot;) + datetime.timedelta(hours=hours, minutes=minutes)).strftime(&quot;%Y-%m-%dT%H:%M:%S&quot;)) if i == 0: preds = fit_model(state, timestamp, initialize=True) else: preds = fit_model(state, timestamp) verts_t = update_verts(preds, bins_t, verts_t) verts_b = update_verts(preds, bins_b, verts_b) date_text.set_text(datetime.datetime.strptime(timestamp, &quot;%Y-%m-%dT%H:%M:%S&quot;).strftime(&quot;%Y-%m-%d %H:%M&quot;)) n_remain = get_votes_remaining(state, timestamp) vote_text.set_text(&quot;{} Votes Remaining&quot;.format(str(n_remain))) mean_text.set_text(&quot;Margin = {:&gt;8} $ pm$ {:&gt;7}&quot;.format(int(np.mean(preds)), int(2.*np.std(preds)))) prob_text.set_text(&quot;Biden win prob = {:.2f}&quot;.format(np.sum(preds &gt; 0)/len(preds))) return [patch_t, patch_b, date_text, vote_text, mean_text, prob_text] def create_animation(state): &quot;Create an animation of the vote updates for the given state&quot; start_time = &quot;2020-11-04T14:00:00&quot; xlim = 100000 ylim = 500 nbins = 200 binsize = xlim//nbins bins_b = np.linspace(0, xlim, binsize, dtype=np.int64) bins_t = np.linspace(-xlim, 0, binsize, dtype=np.int64) verts_t, codes_t = initialize_verts(bins_t) verts_b, codes_b = initialize_verts(bins_b) patch_t = None patch_b = None fig, ax = plt.subplots() barpath_t = path.Path(verts_t, codes_t) patch_t = patches.PathPatch(barpath_t, facecolor=&#39;C3&#39;, edgecolor=&#39;C3&#39;, alpha=0.5) ax.add_patch(patch_t) barpath_b = path.Path(verts_b, codes_b) patch_b = patches.PathPatch(barpath_b, facecolor=&#39;C0&#39;, edgecolor=&#39;C0&#39;, alpha=0.5) ax.add_patch(patch_b) lefttext = -9*xlim//10 righttext = 9*xlim//10 uppertext = 9*ylim//10 lowertext = 8*ylim//10 date_text = text.Text(lefttext, uppertext, &quot;&quot;) ax.add_artist(date_text) vote_text = text.Text(lefttext, lowertext, &quot;&quot;) ax.add_artist(vote_text) prob_text = text.Text(righttext, uppertext, &quot;&quot;, ha=&#39;right&#39;) ax.add_artist(prob_text) mean_text = text.Text(righttext, lowertext, &quot;&quot;, ha=&#39;right&#39;) ax.add_artist(mean_text) ax.set_xlim(-xlim, xlim) ax.set_ylim(0, ylim) ax.set_xlabel(&quot;Biden margin&quot;) ax.set_title(&quot;{} Final Margin Prediction&quot;.format(state)) ani = animation.FuncAnimation(fig, animate, frames=2*24*12, interval=200, fargs=(state, start_time, bins_t, verts_t, patch_t, bins_b, verts_b, patch_b, date_text, vote_text, mean_text, prob_text), repeat=False, blit=True) return ani ani_pa = create_animation(&quot;Pennsylvania&quot;) ani_ga = create_animation(&quot;Georgia&quot;) ani_az = create_animation(&quot;Arizona&quot;) . There was 1 divergence after tuning. Increase `target_accept` or reparameterize. . Displaying these, we can see how the race evolves over time. . from IPython.display import HTML HTML(ani_pa.to_jshtml()) . There were 4 divergences after tuning. Increase `target_accept` or reparameterize. There was 1 divergence after tuning. Increase `target_accept` or reparameterize. . &lt;/input&gt; Once Loop Reflect HTML(ani_ga.to_jshtml()) . There was 1 divergence after tuning. Increase `target_accept` or reparameterize. There was 1 divergence after tuning. Increase `target_accept` or reparameterize. . &lt;/input&gt; Once Loop Reflect HTML(ani_az.to_jshtml()) . There was 1 divergence after tuning. Increase `target_accept` or reparameterize. . &lt;/input&gt; Once Loop Reflect Based on this model, we can see that Pennsylvania was very clearly going in Biden&#39;s direction from early on, despite Trump&#39;s substantial lead at the end of Election Day. This was reflected by comments made by other election data journalists, all of whom were fairly confident that the numbers were good news for Biden even as early as 4 November. Biden&#39;s win probability steadily increased, surpassing 99% on 6 November. The media called the state, and the election, for Biden on 7 November. . Georgia, on the other hand, was not a sure thing. For much of the early data, our model favored Trump, who had a win probability of 82% on the evening of 4 November. However, the uncertainties were wide enough at that point that Biden&#39;s eventual victory was still not an unreasonable outcome. As the ballots shifted towards Biden, we can see a clear change on 5 November, and by that evening Biden&#39;s win probability was 70%. Biden&#39;s chances steadily increased and surpassed 99% on the evening of 7 November. However, since the final margin was still fairly small in absolute terms, the media did not call Georgia until 12 or 13 November. . Arizona, despite being the first state among these that many news outlets called, showed the largest uncertainties for much of the time period we have data, with no candidate having a clear advantage until 9 November when Biden took a slight lead in the model predictions. From there, Biden inched ahead as the remaining ballots came in, and the outcome shifted clearly in his favor on 12 November with his win probability exceeding 99% that evening. The remaining media outlets called Arizona for Biden on 13 November. . As we can see, our model is able to call the outcome of these states slightly before the media does so (possibly due to some level of conservatism). Seeing the range of uncertainties shrink is helpful to know what range of outcomes could still be reasonably expected, and can be a much more interesting way to visualize the results (particularly when animated as above). . Conclusion . This Data Story examined how we could build a Bayesian hierarchical model for the US election data and use it to forecast the final outcome. The model showed how the outcome in three key battleground states evolved over the week following the election as mail ballots were counted, tipping the election in favor of Biden. Because the model includes uncertainties and prior beliefs about voting behavior, this gave a richer picture of how to forecast the final result than simply extrapolating using early returns (with considerable more thought and effort required, however!). Because of the time scales over which the election played out, we could imagine having put this in place to make prospective predictions (stay tuned for 2024!) in real time to see how this simple model aligns with more experienced election forecasters. .",
            "url": "https://alan-turing-institute.github.io/TuringDataStories-fastpages/us%20election/linear%20regression/bayesian%20inference/2021/02/26/Modelling-Mail-In-Votes-In-the-2020-US-Election.html",
            "relUrl": "/us%20election/linear%20regression/bayesian%20inference/2021/02/26/Modelling-Mail-In-Votes-In-the-2020-US-Election.html",
            "date": " • Feb 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Who was protected by the first COVID-19 lockdown in England?",
            "content": "Authors . David Beavan. The Alan Turing Institute. GitHub: @DavidBeavan. Twitter: @DavidBeavan. | Camila Rangel Smith. The Alan Turing Institute. GitHub: @crangelsmith. Twitter: @CamilaRangelS. | Sam Van Stroud. University College London. Github: @samvanstround. | Kevin Xu. Civil Service. Github: @kevinxufs. | . All the authors above contributed to this data story story with ideas, code development and story telling. . Reviewers . Nick Barlow. The Alan Turing Institute. Github: @nbarlowATI | Louise Bowler. The Alan Turing Institute. Github: @LouiseABowler | . Welcome to the first Turing Data Story! . Our goal at Turing Data Stories is to produce educational data science content by telling stories with data. . A data story begins with a question about a societal issue that is close to our hearts, and covers our entire analysis process in trying to answer it. From gathering and cleaning the data, to using it for data analysis. We hope that our stories will not only provide the reader with insight into some societal issues, but also to showcase the explanatory power of data science, and enable the reader to try out some of these techniques themselves. . Each of our stories comes in the form of a Jupyter notebook, which will contain all the code required to follow along with the analysis, along with an explanations of our thought process. . COVID-19 and Deprivation . Everyone has been impacted by the COVID-19 pandemic. On the 23rd of March 2020, the UK Government announced various lockdown measures with the intention of limiting the spread of the virus and reducing the number of COVID-19 related deaths. These lockdown measures meant the temporary closure of many commercial shops and businesses, as well as the limiting of work based travel to only those jobs that could not be done at home. . We are concerned that the impact of COVID-19 has disproportionately affected certain groups of people. In particular, that the lockdown may have had a worse impact on those in the most deprived areas, whose livelihoods may have required them to leave the house more frequently. . There have been a number of concerns with Government COVID-19 reporting, in particular with testing and mortality statistics. This motivates independent, open analysis to validate and expand on our understanding of our current state of the pandemic. . Earlier in June, the Office of National Statistics (ONS) published a report exploring this exact question: to assess whether those living in the most deprived areas of the UK were disproportionately affected by COVID-19. The report seems to confirm our fear - between the months of March to May 2020 those in the most deprived areas of the UK were more than twice as likely to die as a result of COVID-19 than those in the least deprived areas. . There are two caveats that we have with the ONS analysis. The first is reproducibility. We want to confirm the ONS results by making the analysis procedure open. The second caveat is that the ONS report aggregates data over time, and therefore that it might miss interesting differences in outcomes between the different stages of lockdown. The lockdown was most severe between March and May, with measures relaxing from June onwards. We wonder whether the ONS analysis will continue to be relevant as lockdown eases. For this purpose, we wish to extend the ONS analysis to cover all available data, and at the same time, make a comparison between the different stages of lockdown. . Thus for our first story we ask: . Have the COVID-19 lockdown measures protected people equally across all socio-economic groups in society? . We have two main objectives . 1) We want to replicate the ONS analysis using their provided data to ensure that we have all the inputs necessary to understand the problem. . 2) We want to extend the ONS analysis to consider different time periods - representing the severity of the different stages of lockdown - to see how this affects people from different socio-economic groups. . Key Metrics . Our analysis will involve exploring the relationship between the following key metrics: . COVID-19 mortality rates over time and across geographical regions. | Index of multiple deprivation (IMD) by geographical region - a measure of the geographic spread of social deprivation (see definition and explanation). | . Data Sources . We will use the following datasets: . Mortality count time series | IMD Rankings (England only) | Populations provided by the ONS | Local Authority District Code Region Lookup Table provided by the ONS | ONS Mortality and Depravation Data | . In case any of the data sources become unavailable in the future, a download mirror is available here. . For simplicity this study is only focusing on England. We understand the importance of investigating all of the regions of the UK. However due to the difference of lockdown measures across the nations of the UK, and also due to the way the IMD ranking is defined, an independent analysis is required for each nation. We encourage readers to replicate our analysis with the other nations. . Analysis Outline . Here&#39;s a list of the different steps of the analysis: . Download and process data from multiple deprivation and COVID-19 deaths. | Combine the different datasets into a single table by joining on geographical region. | Calculate age standardised mortality rates from mortality counts. | Replicate the ONS analysis, looking at mortality rate by region. | Visualise the distribution of COVID-19 deaths across the UK. | Segment the data into time periods, corresponding to the different stages of lockdown. | Explore at the relationship between our two key metrics (deprivation and mortality rates) in the different time periods. | Data Collation and Wrangling . &#128295; Setup . We begin by setting up our environment and importing various python libraries that we will be using for the analysis. In particular, pandas and numpy are key data science libraries used for data processing. matplotlib and seaborn will help us visualise our data. . import os import requests from datetime import datetime import zipfile import numpy as np import pandas as pd from scipy.stats import pearsonr import matplotlib.pyplot as plt import seaborn as sns sns.set(style=&#39;white&#39;) . 🔧 Let&#39;s make some directories in which we can store the data we are going to download. . downloaded_data_dir = &#39;data/downloaded&#39; # processed data goes here derived_data_dirname = &#39;data/derived&#39; # create the directory if it does not already exist os.makedirs(downloaded_data_dir, exist_ok=True) os.makedirs(derived_data_dirname, exist_ok=True) . 🔧 Here is a small helper function which will download files from a URL. . def download_file(url, filename): # create the directory if it does not already exist os.makedirs(os.path.dirname(filename), exist_ok=True) # make the HTTP request r = requests.get(url, allow_redirects=True) # save file _ = open(filename, &#39;wb&#39;).write(r.content) . Index of Multiple Deprivation (IMD) . 🔧 Now let&#39;s download and process our deprivation data. This data provides a deprivation rank (lower rank meaning more deprived) for each geographical region in England (the geographical regions are here are called Lower Super Output Areas, or LSOAs). As a rough idea of scale, LSOAs contain on average around 1,500 people. . Download . url = &#39;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/833970/File_1_-_IMD2019_Index_of_Multiple_Deprivation.xlsx&#39; # specify filename filename = &#39;ONS_2019_Index_of_Multiple_Deprivation.xlsx&#39; # construct file path filepath = os.path.join(downloaded_data_dir, filename) # download and save file at the specified URL download_file(url, filepath) # read the relevant sheet imd_df = pd.read_excel(filepath, sheet_name=&#39;IMD2019&#39;) . If we sort by deprivation rank, we can get an idea of the most / least deprived LSOAs. . imd_df.sort_values(by=&#39;Index of Multiple Deprivation (IMD) Rank&#39;).head() . LSOA code (2011) LSOA name (2011) Local Authority District code (2019) Local Authority District name (2019) Index of Multiple Deprivation (IMD) Rank Index of Multiple Deprivation (IMD) Decile . 21400 E01021988 | Tendring 018A | E07000076 | Tendring | 1 | 1 | . 12280 E01012673 | Blackpool 010A | E06000009 | Blackpool | 2 | 1 | . 12288 E01012681 | Blackpool 006A | E06000009 | Blackpool | 3 | 1 | . 12279 E01012672 | Blackpool 013B | E06000009 | Blackpool | 4 | 1 | . 12278 E01012671 | Blackpool 013A | E06000009 | Blackpool | 5 | 1 | . imd_df.sort_values(by=&#39;Index of Multiple Deprivation (IMD) Rank&#39;).tail() . LSOA code (2011) LSOA name (2011) Local Authority District code (2019) Local Authority District name (2019) Index of Multiple Deprivation (IMD) Rank Index of Multiple Deprivation (IMD) Decile . 17759 E01018293 | South Cambridgeshire 012B | E07000012 | South Cambridgeshire | 32840 | 10 | . 15715 E01016187 | Bracknell Forest 002D | E06000036 | Bracknell Forest | 32841 | 10 | . 30976 E01031773 | Mid Sussex 008D | E07000228 | Mid Sussex | 32842 | 10 | . 26986 E01027699 | Harrogate 021A | E07000165 | Harrogate | 32843 | 10 | . 17268 E01017787 | Chiltern 005E | E07000005 | Chiltern | 32844 | 10 | . Derive Mean IMD Decile . At this point we want to join the two datasets together in order to explore the relationship between our two key metrics. . A problem is that the index of multiple deprivation comes with a geographical granularity at the LSOA level, whilst the COVID-19 mortality counts come with a geographical granularity at the Local Authority District (LAD) level. To complicate things, for each LAD there are generally multiple LSOAs, each with different indexes of multiple deprivation. For more information about the different geographical regions in the UK, read this. . We need to aggregate the LSOAs into LADs by averaging out the indexes of multiple deprivation. First let&#39;s write some functions to help us. . def get_mean_IMD_decile(LAD_code): # select relevant LSOAs LSOAs = imd_df[imd_df[&#39;Local Authority District code (2019)&#39;] == LAD_code] # calculate mean IMD rank mean_IMD_decile = round(LSOAs[&#39;Index of Multiple Deprivation (IMD) Decile&#39;].mean(), 2) std_IMD_decile = round(LSOAs[&#39;Index of Multiple Deprivation (IMD) Decile&#39;].std(), 2) return mean_IMD_decile, std_IMD_decile . Now we can use these functions to calculate the mean IMD decile in each Local Authority District. . LAD_codes = imd_df[&#39;Local Authority District code (2019)&#39;].unique() mean_IMD_decile, std_IMD_decile = np.vectorize(get_mean_IMD_decile)(LAD_codes) LAD_df = pd.DataFrame({&#39;LAD Code&#39;: LAD_codes, &#39;LAD Name&#39;: imd_df[&#39;Local Authority District name (2019)&#39;].unique(), &#39;Mean IMD decile&#39;: mean_IMD_decile, &#39;Std IMD decile&#39;: std_IMD_decile}) LAD_df = LAD_df.set_index(&#39;LAD Code&#39;) . Let&#39;s make a quick histogram of the mean IMD decile. . LAD_df[&#39;Mean IMD decile&#39;].hist(range=(1,11), bins=10) plt.xlabel(&#39;Mean IMD Decile&#39;) plt.ylabel(&#39;Count&#39;) plt.show() . It should be noted that we lose some information when averaging the IMD ranks in this way. The central region of the distribution is relatively flat, and so we cannot differentiate well between LADs in this region. . Notice there are no Local Authority Districts that have a mean IMD decile of 1 or 10. This is due to the presence of variance inside each Local Authority District. For example, there is no single LAD whose constituent LSOAs all have a IMD deciles of 1 (or 10). See the table below for the maximum and minimum mean IMD deciles. Note that Blackpool, the most deprived (on average) LAD in England, has a mean IMD decile of 2.41. This demonstrates that this LAD has some LSOAs that are not in the most deprived deciles. The opposite is true for the least deprived areas. The &quot;Std IMD decile&quot; column in the below table shows the level of variation of the IMD (measured by the standard deviation) within each LAD. . LAD_df.sort_values(by=&#39;Mean IMD decile&#39;) . LAD Name Mean IMD decile Std IMD decile . LAD Code . E06000009 Blackpool | 2.41 | 1.58 | . E08000003 Manchester | 2.54 | 1.84 | . E08000011 Knowsley | 2.56 | 1.91 | . E09000002 Barking and Dagenham | 2.68 | 1.01 | . E09000012 Hackney | 2.74 | 1.11 | . ... ... | ... | ... | . E07000155 South Northamptonshire | 8.78 | 1.32 | . E07000176 Rushcliffe | 8.82 | 1.58 | . E07000005 Chiltern | 8.86 | 1.51 | . E06000041 Wokingham | 9.27 | 1.43 | . E07000089 Hart | 9.39 | 1.08 | . 317 rows × 3 columns . Derive Age Standardisation Weight . To account for the different population sizes in the different Local Area Districts, we want to use a mortality rate rather than an overall count. When we do this we convert a count into a rate per 100,000 people. Furthermore, we want to account for differences in the age distributions of the different LADs in order to make a valid comparison between the different geographic areas. An age standardised rate allows for this comparison. Ideally we would calculate this rate directly from the data, but as our mortality over time dataset does not contain information about age, we instead will need to extract a standardisation factor from a different dataset. . The dataset we will use to do this comes from the ONS study on COVID-19 and deprivation. We will use it to derive a standardisation factor which will allow us to convert our mortality counts into an age and population standardise mortality rate. This mortality rate is a European standard (2013 ESP). As we mentioned, we cannot calculate the factor directly as our mortality over time dataset does not include age information, so this reverse engineering is the best we can do. . For more information on how the mortality rate is calculated, see here. Simply put, this is the formula that we are assuming approximates the relationship between the age standardised rate and the mortality count: . age standardised mortality rate = [standardisation factor] * [mortality count] . ⚠️ The above procedure is not ideal because it assumes that the distribution of ages of those who died inside each Local Area District is constant in time, and therefore the standardisation factor we derive in one dataset (which doesn&#39;t have information about time) can be applied to the other (which has information about time). . First, let&#39;s download the data. . url = &#39;https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fdeathsinvolvingcovid19bylocalareaanddeprivation%2f1march2020to17april2020/referencetablesdraft.xlsx&#39; # specify filename filename = &#39;ONS_age_standarisation_April2020.xlsx&#39; # construct file path filepath = os.path.join(downloaded_data_dir, filename) # download and save file at the specified URL download_file(url, filepath) # read the relevant sheet age_rate_df = pd.read_excel(filepath, sheet_name=&#39;Table 2&#39;, header=3) . Next, we can do some minor selection and reformatting of the DataFrame. . age_rate_persons_df = age_rate_df[age_rate_df[&#39;Sex&#39;] == &#39;Persons&#39;] # rename columns age_rate_persons_df.columns = [&#39;Sex&#39;, &#39;Geography type&#39;, &#39;LAD Code&#39;, &#39;Area name&#39;, &#39;All causes Deaths&#39;, &#39;All causes Rate&#39;,&#39;&#39; ,&#39;All causes Lower CI&#39;, &#39;All causes Upper CI&#39;,&#39;&#39; ,&#39;COVID-19 Deaths&#39;, &#39;COVID-19 Rate&#39;, &#39;&#39;,&#39;COVID-19 Lower CI&#39;, &#39;COVID-19 Upper CI&#39; ] # remove anomalous row (Isles of Scilly) without numerical data age_rate_persons_df = age_rate_persons_df[age_rate_persons_df[&#39;All causes Rate&#39;] != &#39;:&#39;] age_rate_persons_df = age_rate_persons_df[age_rate_persons_df[&#39;COVID-19 Rate&#39;] != &#39;:&#39;] # remove columns where all entries are NaN due to the spreadsheet formating. age_rate_persons_df.dropna(axis=1) age_rate_persons_df.head() . Sex Geography type LAD Code Area name All causes Deaths All causes Rate All causes Lower CI All causes Upper CI COVID-19 Deaths COVID-19 Rate COVID-19 Lower CI COVID-19 Upper CI . 1 Persons | Unitary Authority | E06000001 | Hartlepool | 154 | 170.7 | NaN | 143.5 | 197.8 | NaN | 29 | 31 | NaN | 20.7 | 44.5 | . 2 Persons | Unitary Authority | E06000002 | Middlesbrough | 289 | 256 | NaN | 226.1 | 286 | NaN | 89 | 79 | NaN | 63.2 | 97.6 | . 3 Persons | Unitary Authority | E06000003 | Redcar and Cleveland | 215 | 142.6 | NaN | 123.5 | 161.8 | NaN | 40 | 26.5 | NaN | 18.9 | 36.2 | . 4 Persons | Unitary Authority | E06000004 | Stockton-on-Tees | 297 | 167 | NaN | 147.8 | 186.1 | NaN | 38 | 21 | NaN | 14.8 | 28.9 | . 5 Persons | Unitary Authority | E06000005 | Darlington | 169 | 151.5 | NaN | 128.6 | 174.4 | NaN | 26 | 22.9 | NaN | 15 | 33.7 | . Let us now calculate the factor by which we need to multiply the count of deaths to derive the age-standardised mortality rate per 100,000 habitants. . age_rate_persons_df[&#39;All causes rate factor&#39;] = ( age_rate_persons_df[&#39;All causes Rate&#39;] / age_rate_persons_df[&#39;All causes Deaths&#39;] ) age_rate_persons_df[&#39;COVID-19 rate factor&#39;] = ( age_rate_persons_df[&#39;COVID-19 Rate&#39;] / age_rate_persons_df[&#39;COVID-19 Deaths&#39;] ) # drop columns age_rate_persons_df = age_rate_persons_df[[&#39;LAD Code&#39;, &#39;All causes rate factor&#39;, &#39;COVID-19 rate factor&#39;]] . We can merge this into the previous DataFrame so all the information is accessible in one place. . LAD_df = LAD_df.reset_index() LAD_df = LAD_df.merge(age_rate_persons_df, on=&#39;LAD Code&#39;, how=&#39;inner&#39;) LAD_df = LAD_df.set_index(&#39;LAD Code&#39;) . Finally, let&#39;s save the standardisation factors for each LAD, stored in the DataFrame LAD_df, so that we can easily use them later. . LAD_df_filename = &#39;Local_Authority_District_Lookup.csv&#39; LAD_df_filepath = os.path.join(derived_data_dirname, LAD_df_filename) # write to csv LAD_df.to_csv(LAD_df_filepath, index=False) . Mortality Counts . Now we are ready to download the main dataset that we will be analysing: the number of COVID-19 and non COVID-19 deaths across time and place. . Download and Format . Let&#39;s download the ONS dataset containing mortality counts by week and Local Authority District. . url = &#39;https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fhealthandsocialcare%2fcausesofdeath%2fdatasets%2fdeathregistrationsandoccurrencesbylocalauthorityandhealthboard%2f2020/lahbtablesweek35.xlsx&#39; # specify filename filename = &#39;ONS_COVID_Mortality_Counts.xlsx&#39; # construct file path filepath = os.path.join(downloaded_data_dir, filename) # download and save file at the specified URL download_file(url, filepath) . sheet_name = &#39;Occurrences - All data&#39; # read the sheet into a pandas DataFrame mortality_df = pd.read_excel(filepath, sheet_name=sheet_name, header=3) . Let&#39;s quickly check if all the LADs are represented in both datasets so that we can join the IMD rank with the mortality information for each LAD. . not_in_imd = set(mortality_df[&#39;Area code&#39;]) - set(imd_df[&#39;Local Authority District code (2019)&#39;]) not_in_mortality = set(imd_df[&#39;Local Authority District code (2019)&#39;]) - set(mortality_df[&#39;Area code&#39;]) print(&#39;There are&#39;, len(not_in_mortality), &#39;codes in the IMD dataset but not in the mortality dataset.&#39;) print(&#39;There are&#39;, len(not_in_imd), &#39;codes in the mortality dataset but not in the IMD dataset.&#39;) . There are 4 codes in the IMD dataset but not in the mortality dataset. There are 30 codes in the mortality dataset but not in the IMD dataset. . We have 346 LAD codes in the mortality data set, and only 317 in the IMD dataset. Upon closer inspection, it turned out that IMD dataset does not contain any Welsh entries (as the IMD ranking is defined for England only). Additionally, the mortality dataset contains a single entry for Buckinghamshire, a new unitary authority in 2020 (E06000060). The IMD dataset, meanwhile, contains 4 LAD codes for Buckinghamshire. We will drop these anomalous locations from the analysis for now. . missing_LAD_codes_df = mortality_df[~mortality_df[&#39;Area code&#39;].isin(imd_df[&#39;Local Authority District code (2019)&#39;])] missing_LAD_codes = missing_LAD_codes_df[&#39;Area code&#39;].unique() # filter by common LAD codes mortality_df = mortality_df[~mortality_df[&#39;Area code&#39;].isin(missing_LAD_codes)] . Furthermore, the age standardisation factor derived previously was not able to be derived for one LAD (the Isles of Scilly). Let&#39;s drop that now too to avoid any problems later down the line. . mortality_df = mortality_df[mortality_df[&#39;Area code&#39;].isin(LAD_df.index)] . Finally, since we are interested in looking at the effect of COVID-19 and the lockdown policies on the working population, we can remove deaths that took place in care homes or hospices. . mortality_df = mortality_df[(mortality_df[&#39;Place of death&#39;] != &#39;Care home&#39;) &amp; (mortality_df[&#39;Place of death&#39;] != &#39;Hospice&#39;)] # to instead select only deaths in care homes or hospices, use this line: #mortality_df = mortality_df[(mortality_df[&#39;Place of death&#39;]==&#39;Care home&#39;) | # (mortality_df[&#39;Place of death&#39;]==&#39;Hospice&#39;)] . The mortality data starts from Wednesday 1st Jan 2020. Let&#39;s use that to convert the supplied week numbers into a date. . mortality_df[&#39;Date&#39;] = [datetime.strptime(f&#39;2020 {n-1} 3&#39;, &#39;%Y %W %w&#39;).strftime(&#39;%Y-%m-%d&#39;) for n in mortality_df[&#39;Week number&#39;]] # drop week number column mortality_df = mortality_df.drop(columns=&#39;Week number&#39;) . Finally, we can take a random sample of 5 rows from the DataFrame to check everything looks okay, and to get an idea of its structure. . mortality_df.sample(n=5) . Area code Geography type Area name Cause of death Place of death Number of deaths Date . 43031 E07000136 | Local Authority | Boston | COVID 19 | Other communal establishment | 0 | 2020-03-11 | . 27659 E08000002 | Local Authority | Bury | COVID 19 | Other communal establishment | 0 | 2020-02-12 | . 41644 E06000043 | Local Authority | Brighton and Hove | All causes | Hospital | 13 | 2020-03-11 | . 28045 E08000036 | Local Authority | Wakefield | All causes | Elsewhere | 2 | 2020-02-12 | . 77533 E09000007 | Local Authority | Camden | All causes | Elsewhere | 0 | 2020-05-06 | . If you want to reproduce the results from the initial ONS report, you can restrict the date ranges of the data by uncommenting these lines. . #mortality_df = mortality_df[mortality_df[&#39;Date&#39;] &lt; &#39;2020-04-18&#39;] . Download Local Area District to Region Lookup Table . As shown in the ONS report, a nice plot to make is the total number of mortalities in each region of England (a region is composed of many LADs). To do this, we need to know which region each LAD belongs. Let&#39;s download this data now from the following website: https://geoportal.statistics.gov.uk/datasets/local-authority-district-to-region-april-2019-lookup-in-england. . url = &#39;https://opendata.arcgis.com/datasets/3ba3daf9278f47daba0f561889c3521a_0.csv&#39; # specify filename filename = &#39;LAD_Code_Region_Lookup.csv&#39; # construct file path filepath = os.path.join (downloaded_data_dir, filename) # download and save file at the specified URL download_file(url, filepath) # read the relevant sheet LAD_code_region_lookup_df = pd.read_csv(filepath, index_col=&#39;FID&#39;).set_index(&#39;LAD19CD&#39;) . Taking a look at the data, we can see that the index &quot;LAD19CD&quot; contains our familiar LAD code, and the column &quot;RGN12NM&quot; gives us the name of the region in which that LAD is located. Perfect! . LAD_code_region_lookup_df.head() . LAD19NM RGN19CD RGN19NM . LAD19CD . E09000001 City of London | E12000007 | London | . E06000054 Wiltshire | E12000009 | South West | . E09000002 Barking and Dagenham | E12000007 | London | . E09000003 Barnet | E12000007 | London | . E09000004 Bexley | E12000007 | London | . Split Data Into Different Time Periods . Now, the final step before we can really start our analysis is to split our dataset into different time periods. As mentioned in the introduction, we want to compare COVID-19 mortality before, during and after lockdown was in place. Within each time period, we sum over all deaths, and add the IMD decile for each LAD. Finally, we&#39;ll also include Region information. . Let&#39;s write a function to sum the mortality data over time to get the total number of deaths. Then, we&#39;ll reformat the DataFrame to separate COVID-19 and non COVID-19 deaths. Finally, we&#39;ll use the table downloaded in the previous section to get the Region name for each LAD. The function combines information from all the previous DataFrames and produces a DataFrame with everything we need to do our analysis. . def filter_date_and_aggregate(df, date_range=None): &quot;&quot;&quot; The function: - Selects dates that are inside the supplied date range. - Sums over time in this date range. - Separates COVID-19 vs non COVID-19 deaths. - Decorates rows with area and region name columns. - Calculates the standardised mortality rate using previously calculated factors. - Pulls in the mean IMD decile as previously calculated. &quot;&quot;&quot; # filter dates if date_range: df = df[(df[&#39;Date&#39;] &gt;= date_range[0]) &amp; (df[&#39;Date&#39;] &lt; date_range[1])] if len(df) == 0: print(&#39;Error: Please make sure there is some data availbile for the supplied date range!&#39;) return None # sum over time df = df.groupby(by=[&#39;Area code&#39;, &#39;Cause of death&#39;]).sum() df = df.reset_index(level=[-1]) # seperate out all deaths and COVID deaths as their own columns df = df.pivot(columns=&#39;Cause of death&#39;, values=&#39;Number of deaths&#39;) df.columns.name = &#39;&#39; # rename columns df = df.rename(columns={&#39;All causes&#39;: &#39;Total deaths&#39;, &#39;COVID 19&#39;: &#39;COVID deaths&#39;}) # add non-COVID deaths as column df[&#39;Non COVID deaths&#39;] = df[&#39;Total deaths&#39;] - df[&#39;COVID deaths&#39;] # add area names df[&#39;Area name&#39;] = LAD_df.loc[df.index][&#39;LAD Name&#39;] # add region names df[&#39;Region name&#39;] = LAD_code_region_lookup_df.loc[df.index][&#39;RGN19NM&#39;] # Calculate the rate per 100k using the age-standardisation factor estimated previously df[&#39;COVID-19 rate&#39;] = (LAD_df.loc[df.index][&#39;COVID-19 rate factor&#39;] * df[&#39;COVID deaths&#39;]).astype(float) df[&#39;All causes rate&#39;] = (LAD_df.loc[df.index][&#39;All causes rate factor&#39;] * df[&#39;Total deaths&#39;]).astype(float) # import mean IMD rank df[&#39;Mean IMD decile&#39;] = LAD_df[&#39;Mean IMD decile&#39;] return df . mortality_sum_df = mortality_df.groupby(by=[&#39;Area code&#39;, &#39;Date&#39;, &#39;Cause of death&#39;]).sum().reset_index() . First, let&#39;s agreggate the data without splitting into different time periods. . # and also includes information about deprivation, no date filtering yet. total_deaths_df = filter_date_and_aggregate(mortality_sum_df) . Now we can split up the data into three periods. The first period is from January 1st to April 7th - 16 days from the beginning of lockdown. The second period runs from April 7th to June 1st - 16 days after the stay at home order was lifted. The final period runs from June 1st to August 28th. . We use a time delay of 16 days after key policy decisions to account for the time lag between onset of the disase and death. The number was taken from this study: &quot;Clinical characteristics of 113 deceased patients with coronavirus disease 2019: retrospective study&quot; BMJ 2020;368:m1091. . first_date_range = (&#39;2020-01-01&#39;, &#39;2020-04-07&#39;) first_df = filter_date_and_aggregate(mortality_sum_df, first_date_range) first_df[&#39;period&#39;] = 1 # second date range second_date_range = (&#39;2020-04-07&#39;, &#39;2020-06-01&#39;) second_df = filter_date_and_aggregate(mortality_sum_df, second_date_range) second_df[&#39;period&#39;] = 2 # second date range third_date_range = (&#39;2020-06-01&#39;, &#39;2020-08-28&#39;) third_df = filter_date_and_aggregate(mortality_sum_df, third_date_range) third_df[&#39;period&#39;] = 3 . print(&#39;Total deaths from COVID-19 in before lockdown period: t&#39;, first_df[&#39;COVID deaths&#39;].sum()) print(&#39;Total deaths from COVID-19 in during lockdown period: t&#39;, second_df[&#39;COVID deaths&#39;].sum()) print(&#39;Total deaths from COVID-19 in after lockdown period: t&#39;, third_df[&#39;COVID deaths&#39;].sum()) . Total deaths from COVID-19 in before lockdown period: 6521 Total deaths from COVID-19 in during lockdown period: 24179 Total deaths from COVID-19 in after lockdown period: 3217 . Let&#39;s also divide the rate by the number of days in each time period, which will give us the the age standardised mortality rate per day. . def get_num_days(date_range): d0 = datetime.strptime(date_range[1], &#39;%Y-%m-%d&#39;) d1 = datetime.strptime(date_range[0], &#39;%Y-%m-%d&#39;) return (d0 - d1).days full_date_range = (&#39;2020-01-01&#39;, &#39;2020-08-28&#39;) total_deaths_df[&#39;All causes rate&#39;] = total_deaths_df[&#39;All causes rate&#39;] / get_num_days(full_date_range) total_deaths_df[&#39;COVID-19 rate&#39;] = total_deaths_df[&#39;COVID-19 rate&#39;] / get_num_days(full_date_range) first_df[&#39;All causes rate&#39;] = first_df[&#39;All causes rate&#39;] / get_num_days(first_date_range) first_df[&#39;COVID-19 rate&#39;] = first_df[&#39;COVID-19 rate&#39;] / get_num_days(first_date_range) second_df[&#39;All causes rate&#39;] = second_df[&#39;All causes rate&#39;] / get_num_days(second_date_range) second_df[&#39;COVID-19 rate&#39;] = second_df[&#39;COVID-19 rate&#39;] / get_num_days(second_date_range) third_df[&#39;All causes rate&#39;] = third_df[&#39;All causes rate&#39;] / get_num_days(third_date_range) third_df[&#39;COVID-19 rate&#39;] = third_df[&#39;COVID-19 rate&#39;] / get_num_days(third_date_range) . all_df = pd.concat([first_df,second_df, third_df]) . Now we are finished with all of the processing steps and we are ready for the fun part: analysing the data! . Study #1 - Regional Mortality Counts &amp; Rates . Our first objective was to reproduce the ONS analysis to ensure that we have all the pieces we need to understand the problem. The first plots that we will replicate compare COVID-19 mortality across the different regions in England (Figures 1 &amp; 2 in the ONS analysis). . For more information about what a &quot;replicate&quot; means in this context, check out the definitions over at the Turing Way. In this case, a replicate means a different team coming to the same conclusion, using the same analysis procedure. . Total Mortalities by Region . For Figure 1, we will produce a stacked bar chart showing the mortality count (split up into COVID-19 and non COVID-19 deaths) for each Region in England. . Summing over region and sorting the values, we are almost ready to make the plot. . total_deaths_by_region_df = total_deaths_df.groupby(by=&#39;Region name&#39;).sum() # sort ascending total_deaths_by_region_df = total_deaths_by_region_df.sort_values(by=&#39;Total deaths&#39;, ascending=True) . Finally, let&#39;s write the code to actually make the plot. . xs = total_deaths_by_region_df.index # mortality counts non_covid_deaths = total_deaths_by_region_df[&#39;Non COVID deaths&#39;] covid_deaths = total_deaths_by_region_df[&#39;COVID deaths&#39;] # set bar width width = 0.75 # colors similar to ONS covid_color = (251/255, 213/255, 59/255, 0.9) noncovid_color = (25/255, 142/255, 188/255, 0.9) # create a figure and plot data plt.figure(figsize=(7,10)) p1 = plt.barh(xs, covid_deaths, width, color=covid_color, label=&#39;COVID-19 Deaths&#39;) p2 = plt.barh(xs, non_covid_deaths, width, left=covid_deaths, color=noncovid_color, label=&#39;Non COVID-19 Deaths&#39;) # label axes plt.xlabel(&#39;Deaths Since 01/01/2020&#39;, fontsize=16) plt.ylabel(&#39;Region&#39;, fontsize=16) plt.yticks(rotation=30) # add vertical grid lines plt.gca().xaxis.grid(True, linestyle=&#39;-&#39;, which=&#39;major&#39;, color=&#39;grey&#39;, alpha=.25) # show legend and plot plt.legend(fontsize=14) plt.show() . There it is! A lot of work but now we can already begin to try and understand what this data is telling us. Here are some conclusions: . For all regions, the number of COVID-19 deaths is smaller than the number of non COVID-19 deaths. | The number of deaths varies a lot between different regions. This can be due to the fact that there are different numbers of people living in each region (for example, there are more people living in the South East than there are in the North East). On top of that, we know that older people have a higher risk of dying after contracting COVID-19, and as the age distributions are different for different regions, this can also affect the overall number of deaths. | . Standardised Mortality Rate by Region . To account for the varying population sizes and age distributions, let&#39;s look at the age-standardised mortality rates per 100,000 people, standardised to the 2013 European Standard Population. Age-standardised mortality rates allow for differences in the age structure of populations and therefore allow valid comparisons to be made between geographical areas, the sexes and over time. . total_rates_df_by_region = total_deaths_df.groupby(by=&#39;Region name&#39;, as_index=False).agg(&#39;mean&#39;) total_rates_df_by_region = total_rates_df_by_region.sort_values(by=&#39;All causes rate&#39;, ascending=True) . x_labels = total_rates_df_by_region[&#39;Region name&#39;] xs = np.array(range(len(x_labels))) # mortality counts non_covid_rate = total_rates_df_by_region[&#39;All causes rate&#39;] covid_rate = total_rates_df_by_region[&#39;COVID-19 rate&#39;] # set bar width width = 0.4 # create a figure and plot data plt.figure(figsize=(7,10)) p2 = plt.barh(xs+0.2, non_covid_rate, width, color=noncovid_color, label=&#39;All Causes Mortality Rate&#39;, tick_label=x_labels) p1 = plt.barh(xs-0.2, covid_rate, width, color=covid_color, label=&#39;COVID-19 Mortality Rate&#39;) # label axes plt.xlabel(&#39;Age standardised mortality rate per 100,000 people per day since 01/01/2020&#39;, fontsize=16) plt.ylabel(&#39;Region&#39;, fontsize=16) plt.yticks(rotation=30) # add vertical grid lines plt.gca().xaxis.grid(True, linestyle=&#39;-&#39;, which=&#39;major&#39;, color=&#39;grey&#39;, alpha=.25) # show legend and plot plt.legend(fontsize=14, loc=&#39;lower right&#39;) plt.show() . Note that as we plot the rates, we switch from a stacked bar chart (showing counts of COVID-19 and non COVID-19 mortalities), to two bar charts side by side (showing the COVID-19 mortality rate, and the all causes mortality rate). Even with this caveat in mind, when looking at the chart we see it tells a very different story to the previous plot. For example, in the previous plot, the South East had the highest number of total deaths, but looking at the standardised rates in this plot we see that it is ranked second from the bottom. This shows that the raw mortality counts do not tell the whole story, and so we cannot rely solely on them to make meaningful comparisons between different Regions. . Study #2 - Mortality by Deprivation . We can now study the relationship between COVID-19 mortality and deprivation. To do this we will make some plots showing the standardised mortality rate as a function of the mean IMD decile for each LAD in England. Recall that the rate has been normalised by the number of days in each time period, so that they are comparable. . In the plots, you will see the $y$ axis labelled with &quot;Standardised Mortality Rate&quot;. In fact, this rate is the age standardised count of deaths observed per 100,000 people per day. . def get_corr_and_slope(xs, ys): # calculate the correlation coefficient corr = round(pearsonr(xs, ys)[0], 2) # calcualte the slope of a linear fit slope = round(np.polyfit(xs, ys, 1)[0], 2) return corr, slope . Comparing All Causes and COVID-19 Mortality Rates . To get started, let&#39;s overlay the mortality rates for all causes of death and COVID-19 on the same plot. . po = {&#39;s&#39;: 10, &#39;alpha&#39;:0.5} # select data IMD_decile = total_deaths_df[&#39;Mean IMD decile&#39;] all_causes_rate = total_deaths_df[&#39;All causes rate&#39;] covid_rate = total_deaths_df[&#39;COVID-19 rate&#39;] # calcualte correlations and slopes ac_stats = get_corr_and_slope(IMD_decile, all_causes_rate) c19_stats = get_corr_and_slope(IMD_decile, covid_rate) # make plots plt.figure(figsize=(10,6)) sns.regplot(x=IMD_decile, y=all_causes_rate, label=f&#39;All causes $r={ac_stats[0]}$&#39;, scatter_kws=po) sns.regplot(x=IMD_decile, y=covid_rate, color=&#39;red&#39;, label=f&#39;COVID-19 $r={c19_stats[0]}$&#39;, scatter_kws=po) # format plot plt.ylabel(&#39;Standardised Mortality Rate&#39;, fontsize=14) plt.xlabel(&#39;Mean IMD Decile&#39;, fontsize=14) plt.legend(fontsize=14); plt.ylim((0, 3.5)) plt.show() . In the plot legends, $r$ is the correlation coefficient. If you are unfamiliar with the concept of correlation, or want a refresher, take a look here. . Recall that the lower the mean IMD rank in each LAD, the more deprived the area is. There is a negative correlation between the standardised rate and the IMD decile, for both the all causes rate and the COVID-19 rate. The negative correlation tells us that in more deprived areas (those with a lower mean IMD decile), the standardised mortality rate is higher. . Note however that the strength of the correlations are quite different. COVID-19 appears to discriminate less based on social deprivation than all causes of death combined. This link between social deprivation and mortality has been previously observed - see discussions here and here. . Time Periods . We are now finally ready to investigate the relationship between mortality and deprivation in the different periods of lockdown. The three periods we defined are: . Before lockdown: January 1st to April 7th | Duing lockdown: April 7th to June 1st | After lockdown: June 1st to August 28th | . We will look at this relationship for all causes of death, and for COVID-19 deaths separately. . All Causes . ymax = 4.5 plt.figure(figsize=(16,5)) # calcualte correlations and slopes pre_stats = get_corr_and_slope(first_df[&#39;Mean IMD decile&#39;], first_df[&#39;All causes rate&#39;]) dur_stats = get_corr_and_slope(second_df[&#39;Mean IMD decile&#39;], second_df[&#39;All causes rate&#39;]) pos_stats = get_corr_and_slope(third_df[&#39;Mean IMD decile&#39;], third_df[&#39;All causes rate&#39;]) plt.subplot(131) plt.title(&#39;Before Lockdown&#39;) sns.regplot(x=&#39;Mean IMD decile&#39;, y=&#39;All causes rate&#39;, data=first_df, label=f&quot;$r={pre_stats[0]}$&quot;, scatter_kws=po) plt.xlabel(&#39;Mean IMD Decile&#39;, fontsize=12) plt.ylabel(&#39;All Causes Standardised Mortality Rate&#39;, fontsize=12) plt.legend(); plt.ylim((0, ymax)) plt.subplot(132) plt.title(&#39;During Lockdown&#39;) sns.regplot(x=&#39;Mean IMD decile&#39;, y=&#39;All causes rate&#39;, data=second_df, label=f&quot;$r={dur_stats[0]}$&quot;, scatter_kws=po) plt.xlabel(&#39;Mean IMD Decile&#39;, fontsize=12) plt.ylabel(&#39;All Causes Standardised Mortality Rate&#39;, fontsize=12) plt.legend(); plt.ylim((0, ymax)) plt.subplot(133) plt.title(&#39;After Lockdown&#39;) sns.regplot(x=&#39;Mean IMD decile&#39;, y=&#39;All causes rate&#39;, data=third_df, label=f&quot;$r={pos_stats[0]}$&quot;, scatter_kws=po) plt.xlabel(&#39;Mean IMD Decile&#39;, fontsize=12) plt.ylabel(&#39;All Causes Standardised Mortality Rate&#39;, fontsize=12) plt.legend(); plt.ylim((0, ymax)) plt.show() . After splitting the data into the three time periods, we that the the negative correlation persists for each period, but has different strengths. The potential reasons for the differences in strength are numerous. The middle plot refers to the peak period in the number of COVID-19 deaths. This could be one explanation for the interesting effect that we observe in the middle plot: the correlation is lower (the variance is larger), but the slope seems to be steeper. . To get a better sense of the number of deaths as a function of time take a look at this ONS study. . COVID-19 . Next we&#39;ll make the same set of three plots, but look specifically at deaths involving COVID-19. . ymax = 2.5 plt.figure(figsize=(16,5)) # calcualte correlations and slopes pre_stats = get_corr_and_slope(first_df[&#39;Mean IMD decile&#39;], first_df[&#39;COVID-19 rate&#39;]) dur_stats = get_corr_and_slope(second_df[&#39;Mean IMD decile&#39;], second_df[&#39;COVID-19 rate&#39;]) pos_stats = get_corr_and_slope(third_df[&#39;Mean IMD decile&#39;], third_df[&#39;COVID-19 rate&#39;]) plt.subplot(131) plt.title(&#39;Before Lockdown&#39;) sns.regplot(x=&#39;Mean IMD decile&#39;, y=&#39;COVID-19 rate&#39;, data=first_df, label=f&quot; t$r={pre_stats[0]}$ $m={pre_stats[1]}$&quot;, scatter_kws=po) plt.xlabel(&#39;Mean IMD Decile&#39;, fontsize=12) plt.ylabel(&#39;COVID-19 Standardised Mortality Rate&#39;, fontsize=12) plt.legend(); plt.ylim((0, 1)) plt.subplot(132) plt.title(&#39;During Lockdown&#39;) sns.regplot(x=&#39;Mean IMD decile&#39;, y=&#39;COVID-19 rate&#39;, data=second_df, label=f&quot; t$r={dur_stats[0]}$ $m={dur_stats[1]}$&quot;, scatter_kws=po) plt.xlabel(&#39;Mean IMD Decile&#39;, fontsize=12) plt.ylabel(&#39;COVID-19 Standardised Mortality Rate&#39;, fontsize=12) plt.legend(); plt.ylim((0, ymax)) plt.subplot(133) plt.title(&#39;After Lockdown&#39;) sns.regplot(x=&#39;Mean IMD decile&#39;, y=&#39;COVID-19 rate&#39;, data=third_df, label=f&quot; t$r={pos_stats[0]}$ $m={pos_stats[1]}$&quot;, scatter_kws=po) plt.xlabel(&#39;Mean IMD Decile&#39;, fontsize=12) plt.ylabel(&#39;COVID-19 Standardised Mortality Rate&#39;, fontsize=12) plt.legend(); plt.ylim((0, 1)) plt.show() . ⚠️ The &quot;During Lockdown&quot; middle plot contains the peak of COVID-19 mortalities in the UK. The standardised rate is therefore much higher in this plot, and the range of the $y$-axis has been increased to reflect this. . Looking only at COVID-19 mortalities, we observe that during lockdown and the peak of the pandemic, the strength of the correlation increases. Again, there are many things that could be the cause of this. Our hypothesis is that, during lockdown, those in more socially deprived areas were more likely to be in circumstances that increased their exposure to COVID-19 (for example key workers who are unable to work remotely - see here and here). . Conclusions . We started writing this story in order to investigate whether COVID-19 deaths were related to social deprivation. We&#39;ve also learnt a lot of other things along the way. . First, there is a very strong relationship between deprivation and general mortality. This means that deprived areas have a higher rate of mortality from all causes than less deprived areas. Life is more difficult if you live in deprivation. This effect is still observed when looking at COVID-19 specific morality rates, although is less strong than the trend from all causes. So COVID-19 appears to be less discriminatory across the different parts of society compared to other causes of death. . We suspected at the beginning that the the state of lockdown may have different affects on different groups of people. We thus wanted to know if this relationship between COVID-19 and death rates changed during the different stages of lockdown. Our analysis shows that the relationship is stronger during the main lockdown period. Note that the relationship we see is only a correlation. More work would need to be done to give any answers about the causal processes that generate this correlation. . There are many things that could be the cause of the observed correlation. Speculatively, it could be down to the fact that people from working class backgrounds are less likely to be able to work from home made them more prone to contracting the virus during lockdown and take it home with them. This would explain their higher rates of mortality. But, as stated, more sophisticated data and analysis techniques would be needed to attempt answer the question of what is the cause of what. . At the moment of writing we are entering a second wave. We hope that we will be able to learn from the challenges that were faced earlier this year, and that more consideration and support is offered to people living in the most deprived areas of the country. . Thank you for taking the time to read this first Turing Data Story, we hope you have found it both interesting and informative. We encourage you to expand on the ideas presented in this story and as well as explore your own questions on this topic. 🙂 . We&#39;re now taking in external submissions for new stories 📗 so if you are interested in contributing, please get in touch! .",
            "url": "https://alan-turing-institute.github.io/TuringDataStories-fastpages/covid-19/data%20wrangling/data%20exploration/2020/11/20/Who_was_protected_by_the_first_COVID-19_lockdown.html",
            "relUrl": "/covid-19/data%20wrangling/data%20exploration/2020/11/20/Who_was_protected_by_the_first_COVID-19_lockdown.html",
            "date": " • Nov 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "Our vision . Our aim is to help people understand the data driven world around us. We want to inspire an open community around a central platform. One that encourages us all to harness the potential of open data by creating ‘data stories’. These ‘data stories’ will mix computer code, narrative, visuals and real world data to document an insightful result. They should relate to society in a way that people care about, and be educational. They must maintain a high standard of openness and reproducibility and be approved by the community in a peer review process. The stories will develop data literacy and critical thinking in the general readership. . About the project . This project was initially formed by a desire to contribute and advance to the analysis of government COVID-19 data. . As part of this process we recognised that government reporting of COVID-19 data was not always in the most accessible format. We also recognised that especially during these times, many invididuals may be interested in developing their technical skills in an impactful way, but not know where to start. . Our goal was therefore to help provide educational data science content that would guide the user through the process of making the data accessible, to using the data for analysis. . We hope that by using the story telling medium, we can bring people along the data science journey and showcase how these techniques can answer both fascinating and socially relevant questions. . What is a Turing Data Story? . The Turing Data Stories should be detailed and pedagogic Jupyter notebooks that document an interesting insight or result using real world data. The aim of the Turing Data Stories is to spark curiosity and motivate more people to play with data. . We expect that the notebook of a data story takes the reader through each step of the analysis done to create the data story results. Turing Data Stories should follow these principles: . The story should be told in a pedagogical way, describing both the context of the story and the methods used in the analysis. The analysis must be fully reproducible (the notebooks should be able to be ran by others using a defined computer environment). The results should be transparent, all data sources are correctly refered to and included. In order to mantain the quality of the results, the Turing Data Story should be peer-reviewed by other contributors before published. We don’t expect sophisticated analyses, just insteresting stories told with data. If you have an idea of a Turing Data Story you want to develop please follow our contributing guidelines to make sure your contributions can be easily integrated in the project. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://alan-turing-institute.github.io/TuringDataStories-fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://alan-turing-institute.github.io/TuringDataStories-fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}